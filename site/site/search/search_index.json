{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tektite","text":""},{"location":"#what-is-tektite","title":"What is Tektite?","text":"<p>Tektite is a powerful Apache Kafka compatible event streaming database that combines the functionality seen in vanilla event streaming platforms such as Apache Kafka or RedPanda with event processing functionality found in platforms such as Apache Flink.</p> <ul> <li>Create Topics just like Kafka or RedPanda. Access them using any Kafka client.</li> <li>Filter, Transform and process data using a powerful expression language and function library.</li> <li>Implement custom processing as WebAssembly modules running in the server</li> <li>Maintain real-time windowed aggregations and materialized views over your data</li> <li>Perform stream/stream and stream/table joins to create new streams</li> <li>Bridge to and from existing external Kafka compatible servers</li> <li>Query the data in any stream or table as if it were a database table</li> </ul> <p>Unlike other offerings, Tektite is not just a bolt on layer over an existing database or event streaming platform.</p> <p>It is designed from first principles to be fast and scale to any size. It contains its own distributed log structured merge tree (LSM) for storage of data. At the low level, data is stored in an object store such Amazon S3 or MinIO.</p> <ul> <li>Learn about Tektite concepts</li> <li>Try the getting started</li> </ul> <p>It's suggested that the rest of the documentation is read in the order it appears in the left hand navigation list.</p>"},{"location":"#current-status","title":"Current Status","text":"<ul> <li>Tektite is currently in active development and working towards a production-ready 1.0 release later in 2024</li> <li>Tektite is usable and development is advanced with most features complete.</li> <li>We will be working heavily on automated testing and performance over the next few months to make sure Tektite is rock-solid and fast for the 1.0 release.</li> </ul>"},{"location":"admin_console/","title":"The admin console","text":"<p>Each Tektite node can host an administration console where you can:</p> <ul> <li>View deployed topics</li> <li>View all deployed streams</li> <li>View database statistics such as number of levels in database, number of tables per level, size of level in bytes, etc.</li> <li>View the current server configuration</li> <li>View cluster information such as active nodes in cluster, which nodes have which processors, etc.</li> </ul>"},{"location":"admin_console/#configuring-the-admin-console","title":"Configuring the admin console","text":"<p>Enable the admin console with the <code>AdminConsoleEnabled</code> configuration property.</p> <p>Set the listen addresses with the <code>AdminConsoleListenAddresses</code> configuration property.</p> <p>To prevent admin requests from taking too much server CPU, the admin console does not get new data every time an admin page is requested. Instead, it samples data according to the<code>AdminConsoleSampleInterval</code> configuration property.</p>"},{"location":"aggregating/","title":"Aggregations","text":"<p>Unlike stateless projections, aggregations apply aggregate functions to the incoming data and the current state to create new state. The state can optionally be grouped by a list of expressions.</p> <p>Tektite supports both non-windowed aggregations and windowed aggregations. </p> <p>Aggregations are defined using the <code>aggregate</code> operator and support the following aggregate functions:</p> <ul> <li><code>count</code> - compute the count of rows seen</li> <li><code>min</code> - compute the minimum of values seen</li> <li><code>max</code> - compute the maximum of values seen</li> <li><code>sum</code> - compute the sum of values seen</li> <li><code>avg</code> - compute the average (mean) of values seen</li> </ul> <p>The operand to the aggregate function can be any valid Tektite expression which evaluates to a type that that is compatible with the aggregate function:</p> <ul> <li><code>count</code> can be used with any type</li> <li><code>min</code> requires operand type <code>int</code>, <code>float</code>, <code>decimal</code>, <code>string</code>, <code>bytes</code>, <code>timestamp</code>. For <code>string</code> and <code>bytes</code>, a lexicographical comparison is performed</li> <li><code>max</code> requires operand type <code>int</code>, <code>float</code>, <code>decimal</code>, <code>string</code>, <code>bytes</code>, <code>timestamp</code>. For <code>string</code> and <code>bytes</code>, a lexicographical comparison is performed</li> <li><code>sum</code> requires operand type <code>int</code>, <code>float</code>, <code>decimal</code></li> <li><code>avg</code> requires operand type <code>int</code>, <code>float</code>, <code>decimal</code>, <code>timestamp</code></li> </ul>"},{"location":"aggregating/#non-windowed-aggregations","title":"Non-windowed aggregations","text":"<p>A non-windowed aggregation computes the aggregate functions over all incoming data received.</p> <p>Here's an example that computes the <code>min</code>, <code>max</code>, <code>count</code> and <code>sum</code> of all sales received so far.</p> <pre><code>sales_totals := sales_topic -&gt;\n    (partition by const partitions = 1) -&gt;\n    (aggregate min(amount), max(amount), count(amount), sum(amount))\n</code></pre> <p>The <code>partition</code> operator is necessary as the incoming data is already partitioned into multiple partitions, but we want to compute a single row over all data, so we need to repartition the incoming data into a single partition before computing the aggregation. If we didn't do this, we'd end up with a totals row per partition.</p> <p>This can then be queried:</p> <pre><code>tektite&gt; (scan all from sales_totals);\n+---------------------------------------------------------------------------------------------------------------------+\n| event_time                 | min(amount)         | max(amount)         | count(amount)        | sum(amount)         |\n+---------------------------------------------------------------------------------------------------------------------+\n| 2024-05-14 07:21:07.851000 | 85.23               | 123.99              | 16                   | 1751.28             |\n+---------------------------------------------------------------------------------------------------------------------+\n</code></pre> <p>You could also store the updates as a new stream - the stream would have a new entry every time the totals were updated:</p> <pre><code>sales_totals_updates := sales_totals -&gt; (store stream);\n</code></pre> <pre><code>tektite&gt; (scan all from sales_totals_updates);\n+--------------------------------------------------------------------------------------------------------------------+\n| offset               | event_time                 | min(amount) | max(amount) | count(amount)        | sum(amount) |\n+--------------------------------------------------------------------------------------------------------------------+\n| 0                    | 2024-05-14 07:36:02.928000 | 85.23       | 123.99      | 17                   | 1836.51     |\n| 1                    | 2024-05-14 07:36:03.926000 | 85.23       | 123.99      | 18                   | 1921.74     |\n| 2                    | 2024-05-14 07:36:04.878000 | 85.23       | 123.99      | 19                   | 2006.97     |\n| 3                    | 2024-05-14 07:36:05.801000 | 85.23       | 123.99      | 20                   | 2092.20     |\n| 4                    | 2024-05-14 07:36:06.624000 | 85.23       | 123.99      | 21                   | 2177.43     |\n| 5                    | 2024-05-14 07:36:07.651000 | 85.23       | 123.99      | 22                   | 2262.66     |\n| 6                    | 2024-05-14 07:36:08.521000 | 85.23       | 123.99      | 23                   | 2347.89     |\n| 7                    | 2024-05-14 07:36:09.377000 | 85.23       | 123.99      | 24                   | 2433.12     |\n+--------------------------------------------------------------------------------------------------------------------+\n8 rows returned\n</code></pre> <p>Or you could expose the update as a Kafka topic, so they can be consumed by any Kafka consumer:</p> <pre><code>sales_totals_updates := sales_totals -&gt; (kafka out);\n</code></pre>"},{"location":"aggregating/#grouping-data-in-aggregations","title":"Grouping data in aggregations","text":"<p>Often, instead of computing a single row in an aggregation, you want to calculate the aggregate functions over subsets of the data.</p> <p>For example, you might want to calculate <code>min</code>, <code>max</code>, <code>count</code> and <code>sum</code> of sales keyed by country. To do this you provide one or more key expressions in the <code>aggregate</code> as follows:</p> <pre><code>sales_totals := sales_topic -&gt;\n    (partition by country partitions = 16) -&gt;\n    (aggregate min(amount), max(amount), count(amount), sum(amount) by country)\n</code></pre> <pre><code>tektite&gt; (scan all from sales_totals);\n+-------------------------------------------------------------------------------------------------------------------+\n| event_time                 | country       | min(amount)   | max(amount)   | count(amount)        | sum(amount)   |\n+-------------------------------------------------------------------------------------------------------------------+\n| 2024-05-14 07:53:54.146000 | uk            | 56.23         | 85.23         | 6                    | 415.38        |\n| 2024-05-14 07:54:35.669000 | usa           | 23.23         | 56.23         | 5                    | 215.15        |\n+-------------------------------------------------------------------------------------------------------------------+\n</code></pre> <p>Note that we repartition the incoming data by the <code>country</code> field as that is what we are grouping by. This ensures all incoming data for the same value of <code>country</code> ends up on the same partition so the aggregation can be calculated correctly.</p> <p>The partition step won't be necessary if the incoming data is already partitioned on the <code>country</code> column.</p> <p>You can also key by multiple expressions:</p> <pre><code>city_totals := sales_topic -&gt;\n    (partition by country, city partitions = 16) -&gt;\n    (aggregate min(amount), max(amount), count(amount), sum(amount) by country, city)\n</code></pre> <pre><code>tektite&gt; (scan all from city_totals);\n+------------------------------------------------------------------------------------------------------------------------------+\n| event_time                 | country      | city         | min(amount)  | max(amount)  | count(amount)        | sum(amount)  |\n+------------------------------------------------------------------------------------------------------------------------------+\n| 2024-05-14 07:53:54.146000 | uk           | london       | 76.23        | 85.23        | 3                    | 246.69       |\n| 2024-05-14 07:53:44.438000 | uk           | manchester   | 56.23        | 56.23        | 3                    | 168.69       |\n| 2024-05-14 07:54:24.980000 | usa          | austin       | 23.23        | 23.23        | 2                    | 46.46        |\n| 2024-05-14 07:54:35.669000 | usa          | miami        | 56.23        | 56.23        | 3                    | 168.69       |\n+------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"aggregating/#windowed-aggregations","title":"Windowed aggregations","text":"<p>Very commonly, you don't want to compute aggregations over all data but over some window, e.g. in a window of a day or an hour. You accomplish this with a windowed aggregation. </p> <p>A windowed aggregation is similar to a non-windowed aggregation but has extra parameters to define the window <code>size</code> and <code>hop</code>.</p> <p>Window <code>size</code> is a duration that represents how long a window stays open, for example, if you wanted to aggregate sales by hour, the window size would be 1 hour.</p> <p>Window <code>hop</code> is also a duration that represents the time between opening new windows. If <code>hop</code> is equal to <code>size</code> then at any particular time we only have one window open. This is known in other systems as a hopping window.</p> <p>If <code>hop</code> is less than size, then we can have multiple open windows at any one time. For example, we might have <code>size</code> set  to 1 hour, and <code>hop</code> set to 10 minutes. This gives us more timely results as we don't have to wait an hour to get the most recent sales results, we will get updated results every 10 minutes.</p> <p>Here's an example of the sales aggregation from before, but this time using a windowed aggregation:</p> <pre><code>sales_by_country_by_hour := sales_topic -&gt;\n    (partition by country partitions = 16) -&gt;\n    (aggregate min(amount), max(amount), count(amount), sum(amount)\n        by country size = 1h hop = 10m)\n</code></pre> <p>Note that durations are expressed as a positive integer followed by one of <code>ms</code> for milliseconds, <code>s</code> for seconds, <code>m</code> for minutes, <code>h</code> for hours or <code>d</code> for days.</p> <p>Tektite maintains aggregate values for each open window, and results are emitted and visible when the window is closed. How do we determinate that?</p>"},{"location":"aggregating/#watermarks","title":"Watermarks","text":"<p>Tektite uses watermarks to determine when aggregate windows can be closed.</p> <p>At any one time, there can be multiple windows open for an aggregation depending on the values of <code>size</code> and <code>hop</code>.</p> <p>As data arrives at the <code>aggregate</code> operator it first determines which window(s) the event intersects with. Each window is defined by the start time of the window <code>ws</code> and the end time of the window <code>we</code>.</p> <p>A row intersects with a window if <code>ws</code> &lt; <code>event_time</code> &lt;= <code>we</code>. Each incoming row has an <code>event_time</code> column.</p> <p>For each intersecting window, the aggregate expressions are evaluated for the incoming row.</p> <p>It's not only rows that flow through the network of interconnecting streams in Tektite. Tektite also injects watermarks at the places where data enters the system - <code>bridge from</code> and <code>kafka in</code> operators, and these flow through the streams along all the routes that data can flow.</p> <p>A watermark carries a timestamp with it. This timestamp means that no more data with an <code>event_time</code> less than this timestamp is expected to flow through the operator. </p> <p>When the <code>aggregate</code> operator receives a watermark it can close all open windows whose <code>we</code> timestamp is &lt;= the watermark timestamp.</p> <p>In some systems, events can arrive late, and we might not want to close the window until some time after the watermark, this  is done by specifying the <code>lateness</code> parameter.</p> <p>Here we don't close open windows until 10 seconds after the timestamp carried by the watermark.</p> <pre><code>sales_by_country_by_hour := sales_topic -&gt;\n    (partition by country partitions = 16) -&gt;\n    (aggregate min(amount), max(amount), count(amount), sum(amount)\n        by country size = 1h hop = 10m lateness = 10s)\n</code></pre> <p>By default, watermarks are injected in <code>bridge from</code> and <code>kafka in</code> operators approximately 1 second after the <code>event_time</code> of the highest <code>event_time</code> seen on incoming data. This can be configured on those operators. For more information on watermarks see the section on generating watermarks</p> <p>When a window is closed the aggregate values for the last closed window becomes visible and can be queried:</p> <pre><code>tektite&gt; (scan all from sales_by_country_by_hour);\n+-----------------------------------------------------------------------------------------------------------------------------------------------+\n| event_time                 | country              | min(amount)          | max(amount)          | count(amount)        | sum(amount)          |\n+-----------------------------------------------------------------------------------------------------------------------------------------------+\n| 2024-05-14 10:51:17.962000 | uk                   | 56.23                | 85.23                | 5                    | 339.15               |\n| 2024-05-14 10:51:29.687000 | usa                  | 23.23                | 23.23                | 2                    | 46.46                |\n+-----------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre> <p>Note that the aggregate results are keyed on <code>country</code> so we only store the most recent aggregate results per country. The <code>event_time</code> column represents the highest <code>event_time</code> for any row which contributed towards that row of the aggregate results.</p> <p>We could also create a new stream that receives updates from the aggregation or expose it as a Kafka consumer endpoint or siphon the results off to an external topic.</p> <p>Sometimes you may want to retain aggregation results from all closed windows, not just the most recent one. You do this by specifying the <code>window_cols</code> parameter set to <code>true</code>. </p> <pre><code>sales_by_country_by_hour := sales_topic -&gt;\n    (partition by country partitions = 16) -&gt;\n    (aggregate min(amount), max(amount), count(amount), sum(amount)\n        by country size = 1h hop = 10m window_cols = true)\n</code></pre> <p>Then closed windows will be output with additional columns <code>ws</code> and <code>we</code> and the key of the data will be <code>[ws, we, country]</code></p> <p>By default, a windowed aggregation stores results persistently in a table with the same name as the stream, when windows are closed. Sometimes you don't want results to be stored, you just want to emit results somewhere else. For example you might want to send results to an external Kafka topic or expose results as a Kafka topic in Tektite.</p> <p>You set the <code>store</code> parameter to <code>false</code> to prevent the aggregate storing results.</p> <p>Here's an example of creating a new (read-only) topic that exposes the latest aggregate sales figures from the existing topic <code>sales_topic</code>:</p> <pre><code>sales_by_country_by_hour_topic := sales_topic -&gt;\n    (partition by country partitions = 16) -&gt;\n    (aggregate min(amount), max(amount), count(amount), sum(amount)\n        by country size = 1h hop = 10m store = false) -&gt;\n    (kafka out)\n</code></pre> <p>And here's an example of computing latest aggregate sales figures and outputting them directly to an external topic in Apache Kafka</p> <pre><code>sales_by_country_by_hour_out := sales_topic -&gt;\n    (partition by country partitions = 16) -&gt;\n    (aggregate min(amount), max(amount), count(amount), sum(amount)\n        by country size = 1h hop = 10m store = false) -&gt;\n    (bridge to external_sales_figures props = (\"bootstrap.servers\" = \"foo.com:9092\")))\n</code></pre> <p>Here's an example of computing latest aggregate sales figures without persisting them and using a <code>store table</code> operator to explicitly persist the results in a table. </p> <pre><code>sales_by_country_by_hour_with_table := sales_topic -&gt;\n    (partition by country partitions = 16) -&gt;\n    (aggregate min(amount), max(amount), count(amount), sum(amount)\n        by country size = 1h hop = 10m store = false) -&gt;\n    (store table by country)\n</code></pre>"},{"location":"aggregating/#data-retention","title":"Data retention","text":"<p>If you don't want to keep the results of your aggregation forever, you can set a maximum retention time on it. Data will be deleted asynchronously from the aggregation once that time has been exceeded.</p> <p>The following will delete the aggregate results after 1 day:</p> <pre><code>sales_by_country_by_hour := sales_topic -&gt;\n    (partition by country partitions = 16) -&gt;\n    (aggregate min(amount), max(amount), count(amount), sum(amount)\n        by country size = 1h hop = 10m retention = 1d)\n</code></pre>"},{"location":"backfill/","title":"Back-filling streams","text":"<p>Let's say you have an existing stream which already contains data, in this case a topic:</p> <pre><code>my-topic := (topic partitions = 16)\n</code></pre> <p>And you want to create a child stream of <code>my-topic</code>:</p> <pre><code>child-stream := my-topic -&gt; (filter by len(val) &gt; 1000) -&gt; (store stream)\n</code></pre> <p>When <code>child-stream</code> is created it won't receive any of the messages in <code>my-topic</code> that were there before it was created. It will only receive new messages that arrive after it was created.</p> <p>Sometimes you want to run all the existing data into the child stream. You do this with a <code>backfill</code> operator:</p> <pre><code>child-stream := my-topic -&gt; (backfill) -&gt;\n    (filter by len(val) &gt; 1000) -&gt; (store stream)\n</code></pre> <p>The <code>backfill</code> operator will scan all the data in the parent stream and feed it into the child stream before the child stream receives any new messages.</p> <p>Please note, that if the parent stream has a lot of data, it can take a significant time to back-fill the child stream</p>"},{"location":"bridging/","title":"Bridging to and from external topics","text":"<p>You can connect a stream to an external topic that lives in any Kafka compatible server - it could an existing Apache Kafka or RedPanda instance, or another Tektite cluster. We call this bridging to an external topic. Bridges can be set up to consume messages from an external topic into the stream or to produce messages from the stream to the external topic.</p> <p>Bridges are resilient; they cope with temporary unavailability of the external topic, reconnecting automatically when the server is available again.</p>"},{"location":"bridging/#bridging-from-an-external-topic","title":"Bridging from an external topic","text":"<p>To bridge from an external topic and consume messages into your stream you use the <code>bridge from</code> operator. This operator will internally maintain a set of Kafka message consumers which consume messages and make them available in your stream.</p> <p>Here's an example of creating a stream that consumes from an external topic called <code>external-topic</code>.</p> <p>The messages contain JSON which have a string field called <code>country</code>. The stream filters out any messages which don't have country == <code>UK</code> then exposes the stream as a Tektite (read-only) topic which can be consumed by any Kafka consumer.</p> <pre><code>uk_sales :=\n    (bridge from external-sales partitions = 16\n        props = (\"bootstrap.servers\" =\n                     \"mykafka1.foo.com:9092, mykafka2.foo.com:9092\")) -&gt;\n    (filter by json_string(\"country\", val) == \"UK\") -&gt;\n    (kafka out)\n</code></pre> <p>Here's another example, where we consume from an external topic and maintain a windowed aggregation which captures total number and value of sales per country in a one-hour window with a 10-minute hop.</p> <pre><code>uk_sales_totals :=\n    (bridge from external-sales partitions = 16\n        props = (\"bootstrap.servers\" =\n                     \"mykafka1.foo.com:9092, mykafka2.foo.com:9092\")) -&gt;\n    (project json_string(\"country\", val) as country,\n             to_decimal(json_string(\"value\", val), 10, 2) as value) -&gt;\n    (partition by country partitions = 10) -&gt;\n    (aggregate count(value), sum(value) by country size = 1h hop = 10m)\n</code></pre> <p>Note that <code>partitions</code> must be specified and must correspond to the number of partitions in the external topic.</p> <p>The <code>props</code> parameter is used to provide properties to the Kafka client. The <code>bootstrap.servers</code> property, at minimum must be provided. This must contain the addresses of one or more Kafka servers that host the external topic in a comma separated list.</p> <p>By default, only messages arriving at the external topic after the <code>bridge from</code> was created will be consumed. If you want to consume all existing messages in the external topic you can set the <code>auto.offset.reset</code> property to <code>earliest</code>. The default value is <code>latest</code>.</p> <p>The <code>bridge from</code> operator has some parameters that can be set:</p> <ul> <li><code>max_poll_messages</code>: Maximum number of messages to consume (if available) and pass to the stream in a single batch. Defaults to <code>1000</code></li> <li><code>poll_timeout</code>: Maximum time to wait polling for messages to arrive before polling again. Defaults to <code>50ms</code></li> </ul> <p>The <code>bridge from</code> also generates watermarks.</p>"},{"location":"bridging/#bridging-to-an-external-topic","title":"Bridging to an external topic","text":"<p>To send messages from your stream to an external topic you use the <code>bridge to</code> operator.</p> <p>The <code>bridge to</code> will automatically reconnect to the external server after unavailability. If the server is unavailable pending messages to send will be stored in Tektite so the local stream can continue operating. When the server becomes available again, pending messages will be sent.</p> <p>Here's an example of bridging from a local stream to an external topic called <code>external-topic</code></p> <pre><code>out-stream := cust-updates -&gt; (filter by area == \"USA\") -&gt;\n    (bridge to external-topic props =\n        (\"bootstrap.servers\" = \"mykafka1.foo.com:9092, mykafka2.foo.com:9092\"))\n</code></pre> <p>Here's an example of a local write-only Tektite topic which immediately forwards its messages on to an external topic:</p> <pre><code>sales-send-proxy := (topic partitions = 16) -&gt;\n    (bridge to sales props =\n        (\"bootstrap.servers\" = \"mykafka1.foo.com:9092, mykafka2.foo.com:9092\"))\n</code></pre> <p>Please note, that the partition number is maintained when sending to the external topic.</p> <p>I.e. if a message is in partition 12 in the local stream it will be sent to partition 12 in the external topic, so you must ensure that the number of partitions in your local stream matches the number of partitions in the external topic. If it doesn't you should repartition your stream before sending.</p> <p>For example, if the external topic has 50 partitions:</p> <pre><code>out-stream := cust-updates -&gt; (filter by area == \"USA\") -&gt;\n    (partition by customer_id partitions = 50) -&gt;\n    (bridge to external-topic props =\n        (\"bootstrap.servers\" = \"mykafka1.foo.com:9092, mykafka2.foo.com:9092\"))\n</code></pre> <p>The <code>bridge to</code> operator has some parameters that can be set:</p> <ul> <li><code>retention</code>: Maximum time to keep locally stored messages. Default is <code>24h</code></li> <li><code>initial_retry_delay</code>: When target server is unavailable how long to wait before initially retrying to connect. Default is <code>5s</code></li> <li><code>max_retry_delay</code>: When reconnecting, retry delay automatically increases up to a maximum of this value. Default is <code>30s</code></li> <li><code>connect_timeout</code>: How long to wait for a connection before considering it failed. Default is <code>5s</code></li> <li><code>send_timeout</code>: How long to wait for sending a batch to complete before considering it failed. Default is <code>2s</code></li> </ul>"},{"location":"cli/","title":"The command line interpreter (CLI)","text":"<p>This guide assumes you have installed Tektite and the Tektite binaries directory is on the <code>PATH</code> environment variable</p> <p>You interact with Tektite at the command line using the <code>tektite</code> command.</p> <p>To see the options, use <code>tektite --help</code>:</p> <pre><code>&gt; tektite --help\nUsage: tektite\n\nFlags:\n  -h, --help                         Show context-sensitive help.\n      --address=\"127.0.0.1:7770\"     Address of tektite server to connect to.\n      --trusted-certs-path=STRING    Path to a PEM encoded file containing certificate(s) of trusted servers and/or certificate authorities\n      --key-path=STRING              Path to a PEM encoded file containing the client private key. Required with TLS client authentication\n      --cert-path=STRING             Path to a PEM encoded file containing the client certificate. Required with TLS client authentication\n      --no-verify                    Set to true to disable server certificate verification. WARNING use only for testing, setting this can expose you to\n                                     man-in-the-middle attacks\n      --command=STRING               Single command to execute, non interactively\n</code></pre> <p>The CLI can be run both interactively (as a shell) and non interactively, for executing single commands</p>"},{"location":"cli/#connecting","title":"Connecting","text":"<p>You specify the address of the server to connect to with the <code>--address</code> option. This defaults to <code>127.0.0.1:7770</code> which is an address a local development server using the <code>standalone.cfg</code> would be listening at.</p> <p>The CLI uses the Tektite HTTP API, so the address to connect to corresponds to the address the HTTP API listens at. That is configured on the server with the <code>http-api-addresses</code> configuration property.</p>"},{"location":"cli/#connecting-to-server-using-self-signed-certificates","title":"Connecting to server using self-signed certificates","text":"<p>If your Tektite HTTP API server is using self-signed certificates then will need to specify a path to a PEM file containing the trusted certificates or certificate authorities on the command line:</p> <pre><code>tektite --address tektite.foo.com:7770 --trusted-certs-path path/to/mycerts.pem\n</code></pre>"},{"location":"cli/#connecting-using-client-certificates","title":"Connecting using client certificates","text":"<p>If you've configured your server to expect client certificates when connecting, you must provide you must paths to PEM files containing the client private key and client certificate:</p> <pre><code>tektite --address tektite.foo.com:7770 --key-path /path/to/myclientkey.pem --cert-path /path/to/myclientcert.pem\n</code></pre>"},{"location":"cli/#using-no-verify","title":"Using <code>no-verify</code>","text":"<p>When working with a local Tektite server for development and demos you can use the <code>--no-verify</code> option which prevents the client from verifying whether server certificates are trusted.</p>"},{"location":"cli/#executing-single-commands","title":"Executing single commands","text":"<p>You can run single commands with <code>tektite</code> using the <code>command</code> option:</p> <p>For example, to create a topic:</p> <pre><code>tektite --command \"my-topic := (topic partitions = 16)\"\nOK\n</code></pre> <p>And create another one:</p> <pre><code>tektite --command \"other-topic := (topic partitions = 16)\"\nOK\n</code></pre> <p>List all streams:</p> <pre><code>tektite --command \"list()\"\n+----------------------------------------------------------------------------------------------------------------------+\n| stream_name                                                                                                          |\n+----------------------------------------------------------------------------------------------------------------------+\n| my-topic                                                                                                             |\n| other-topic                                                                                                          |\n+----------------------------------------------------------------------------------------------------------------------+\n2 rows returned\n</code></pre> <p>Execute a query:</p> <pre><code>tektite --command \"(scan all from my-topic)\"\n+---------------------------------------------------------------------------------------------------------------------+\n| offset               | event_time                 | key                 | hdrs                | val                 |\n+---------------------------------------------------------------------------------------------------------------------+\n0 rows returned\n</code></pre>"},{"location":"cli/#using-tektite-interactively","title":"Using <code>tektite</code> interactively","text":"<p>If the <code>command</code> option is omitted, <code>tektite</code> will run interactively, as a shell, allowing you to input multiple commands:</p> <p>You can enter commands over multiple lines, and terminate them with semicolon ';' followed by newline.</p> <pre><code>tektite --address tektite.foo.com:7770\ntektite&gt; list();\n+----------------------------------------------------------------------------------------------------------------------+\n| stream_name                                                                                                          |\n+----------------------------------------------------------------------------------------------------------------------+\n| my-topic                                                                                                             |\n| other-topic                                                                                                          |\n+----------------------------------------------------------------------------------------------------------------------+\n2 rows returned\ntektite&gt; foo-stream :=\n         my-topic -&gt;\n             (project key, to_upper(to_string(val))) -&gt;\n             (store stream);\nOK\ntektite&gt;             \n</code></pre> <p>Let's take a look at the supported commands</p>"},{"location":"cli/#creating-topics-and-streams","title":"Creating topics and streams","text":"<p>You can create streams using the standard syntax:</p> <pre><code>tektite&gt; my-topic := (topic partitions = 32);\nOK\ntektite&gt; my-other-topic := (topic partitions = 16);\nOK\ntektite&gt; filtered-stream := bar-topic -&gt; (filter by len(val) &gt; 1000) -&gt; (store stream);\nOK\n</code></pre>"},{"location":"cli/#listing-streams","title":"Listing streams","text":"<p>Use <code>list()</code> to list all streams:</p> <pre><code>tektite&gt; list();\n+----------------------------------------------------------------------------------------------------------------------+\n| stream_name                                                                                                          |\n+----------------------------------------------------------------------------------------------------------------------+\n| filtered-stream                                                                                                      |\n| my-other-topic                                                                                                       |\n| my-topic                                                                                                             |\n+----------------------------------------------------------------------------------------------------------------------+\n3 rows returned\n</code></pre> <p>Use <code>list</code> with a regular expression to list all streams whose name matches the regular expression:</p> <p>E.g. list all streams whose names start with <code>my</code></p> <pre><code>tektite&gt; list(\"^my\");\n+----------------------------------------------------------------------------------------------------------------------+\n| stream_name                                                                                                          |\n+----------------------------------------------------------------------------------------------------------------------+\n| my-other-topic                                                                                                       |\n| my-topic                                                                                                             |\n+----------------------------------------------------------------------------------------------------------------------+\n2 rows returned\n</code></pre>"},{"location":"cli/#showing-a-stream","title":"Showing a stream","text":"<p>Use <code>show(&lt;stream_name&gt;)</code> to show details of a stream:</p> <pre><code>tektite&gt; show(my-topic);\n\nstream_name:   my-topic\nstream_def:    (topic partitions = 32)\nin_schema:     {record_batch: bytes} partitions: 32 mapping_id: _default_\nout_schema:    {offset: int, event_time: timestamp, key: bytes, hdrs: bytes, val: bytes} partitions: 32 mapping_id: _default_\nchild_streams: filtered-stream\n</code></pre> <p>The information shows:</p> <ul> <li><code>stream_name</code>: This is the name of the stream</li> <li><code>stream_def</code>: This is the TSL command used to define the stream</li> <li><code>in_schema</code>: This is the schema that the stream receives. Number of partitions and mapping_id is also shown.</li> <li><code>out_schema</code>: This is the schema that the stream outputs. Number of partitions and mapping_id is also shown.</li> <li><code>child_streams</code>: the names of any child streams.</li> </ul>"},{"location":"cli/#executing-ad-hoc-queries","title":"Executing ad-hoc queries","text":"<p>You can execute ad-hoc queries at the command line.</p> <pre><code>tektite&gt; (scan all from my-topic);\n+---------------------------------------------------------------------------------------------------------------------+\n| offset               | event_time                 | key                 | hdrs                | val                 |\n+---------------------------------------------------------------------------------------------------------------------+\n| 0                    | 2024-05-18 06:33:59.168000 | apples              | .                   | quwhdqiuwhdiquwhd.. |\n| 0                    | 2024-05-18 06:34:02.451000 | oranges             | .                   | wueiuqwediuqwduiqwd |\n| 0                    | 2024-05-18 06:34:06.450000 | pears               | .                   | uiwqbuqwdiuqwdqwd   |\n+---------------------------------------------------------------------------------------------------------------------+\n3 rows returned\n</code></pre>"},{"location":"cli/#deleting-streams","title":"Deleting streams","text":"<p>Streams are deleted with the <code>delete</code> command:</p> <pre><code>tektite&gt; delete(my-other-topic);\nOK\n</code></pre>"},{"location":"cli/#exiting-the-cli","title":"Exiting the CLI","text":"<p>CTRL-C to exit</p>"},{"location":"cli/#setting-max-line-width","title":"Setting <code>max-line-width</code>","text":"<p>In interactive mode the CLI will try to format all results with a max width which has a default of 120 characters. You can change this, e.g. if you have a lot of columns to display with <code>set max-line-width</code></p> <pre><code>tektite&gt; set max_line_width 200;\nOK\n</code></pre>"},{"location":"cli/#registering-and-unregistering-webassembly-modules","title":"Registering and unregistering WebAssembly modules","text":"<p>You can use the CLI to register and unregister WebAssembly modules.</p> <p>See the chapter on WebAssembly for more information.</p> <pre><code>tektite&gt; register_wasm(\"path/to/my/modules/my_module.wasm\");\nOK\ntektite&gt; unregister_wasm(\"my_module\");\nOK\n</code></pre>"},{"location":"conceptual_model/","title":"Conceptual Model","text":"<p>Event Streaming platforms such as Apache Kafka or Red Panda and event processing platforms such as Apache Flink are traditionally seen as separate beasts, but fundamentally both just manage streams of data.</p> <p>Tektite exposes the stream processing primitives, so they can be assembled together in different ways to create all the things you find in event streaming and event processing platforms, but in a single unified platform.</p> <p>Wouldn\u2019t it be nice if you could deploy a topic, but also with some filtering or processing happening on the server? Or to join two Kafka topics with a one-liner, and avoid spinning up a separate Flink cluster to do this? Or maybe you want to run some custom processing on your Kafka cluster?</p> <p>It's like a Swiss Army Knife for event streaming. The language that Tektite uses to define streams is called Tektite Streaming Language (TSL).</p> <p>Let's discuss the conceptual model of Tektite in more detail.</p>"},{"location":"conceptual_model/#streams","title":"Streams","text":""},{"location":"conceptual_model/#creating-streams","title":"Creating streams","text":"<p>A stream is a sequence of operators. Data flows from left to right, from one operator to the next.</p> <p>Here's an example of creating a new stream, this stream is just a vanilla Kafka compatible topic:</p> <pre><code>my-topic := (topic partitions = 16)\n</code></pre> <p>Here's another stream, let's look at the syntax in more detail:</p> <pre><code>my-stream := (kafka in) -&gt; (filter by len(value) &gt; 1000) -&gt; (kafka out)\n</code></pre> <p>Here, we create a new stream called <code>my-stream</code> and it's composed of three operators. There's a <code>kafka in</code> operator,  followed by a <code>filter</code> operator, followed by a <code>kafka out</code> operator. Operators are chained together using an arrow <code>-&gt;</code> - the arrow represents the flow of data from one operator to the next.</p> <p>Each operator is enclosed in parentheses. Inside the parentheses you will first find the operator name followed by any specific arguments the operator takes. That's basically it!</p> <p>Tektite comes with various different operators that you can use to compose streams that do many different things. We'll discuss the different types of operators in detail in other sections of the documentation.</p> <p>In this example, the <code>kafka in</code> operator exposes a Kafka topic endpoint to the outside world that receives produced messages. I.e. you can produce messages to it from any Kafka compatible client. The <code>filter</code> operator only passes through rows where the expression evaluates to <code>true</code>. The <code>kafka out</code> operator exposes the stream, so it can be consumed by any Kafka consumer.</p> <p>So, what does this stream do? It's a stream that accepts produced messages but filters out messages where the length of the value is less than or equal 1000 bytes.</p> <p>Often, a stream takes its input from the output of another stream. In that case the first operator in the new stream is replaced with the name of the stream that feeds it.</p> <pre><code>child_stream := my-stream -&gt; (aggregate sum(foo) by country)\n</code></pre> <p>Because streams can feed into other streams, the network of streams forms a graph.</p>"},{"location":"conceptual_model/#deleting-streams","title":"Deleting streams","text":"<p>You delete streams with the <code>delete</code> command which takes the stream name as an argument.</p> <pre><code>delete(my_stream);\n</code></pre> <p>You won't be allowed to delete a stream if it has child streams as that would result in orphans. You'll have to delete the child streams first.</p>"},{"location":"conceptual_model/#queries","title":"Queries","text":"<p>A query takes data from a source, e.g. by scanning a table or stream or performing a lookup and then passes it through a sequence of operators which transform the data in some way.</p> <p>Sounds just like a stream again! In Tektite, we also represent a query as a stream which is fed data from its source, does some processing on it, then outputs the query results at the end. Queries are discussed in detail in the section on queries.</p> <p>A couple of examples:</p> <p>Scan the <code>cust-data</code> table from <code>\"smith\"</code> to <code>\"wilson\"</code> then pass through the <code>cust_name</code> (after upper-casing it), <code>age</code> and <code>dob</code> columns. Filter out rows where age &lt;= 30, then sort the results by <code>dob</code></p> <pre><code>(scan \"smith\" to \"wilson\" from cust_data) -&gt;\n(project to_upper(cust_name), age, dob) -&gt;\n(filter by age &gt; 30) -&gt;\n(sort by dob)\n</code></pre> <p>A simple lookup based on key and pass through the <code>tx_id</code>, <code>cust_id</code> and <code>amount</code> columns.</p> <pre><code>(get tx1234 from transactions) -&gt; (project tx_id, cust_id, amount)\n</code></pre>"},{"location":"conceptual_model/#operators","title":"Operators","text":"<p>Operators receive batches of data and output batches of data. Operators are composed into streams as detailed above.</p> <p>Some operators such as <code>kafka in</code> and <code>bridge from</code> accept data from outside of Tektite, and have no previous operator, so they can only appear as the first operator in a stream.</p> <p>Other operators such as <code>kafka out</code> and <code>bridge to</code> send data outside of Tektite and have no next operator, so they can only appear as the last operator in a stream.</p> <p>Tektite operators include:</p> <ul> <li><code>topic</code> - exposes a Kafka producer endpoint and a Kafka consumer endpoint - i.e. it creates a Kafka compatible topic. This is really shorthand for <code>(kafka in) -&gt; (kafka out)</code>, but its such a common thing to do we provide a distinct operator for it.</li> <li><code>kafka in</code> - exposes a Kafka endpoint that accepts messages from any Kafka producer.</li> <li><code>kafka out</code> - exposes a Kafka endpoint that can be consumed by any Kafka consumer.</li> <li><code>project</code> - evaluates a list of expressions over the incoming batch to create a new batch. Expressions use a powerful expression language and function library.</li> <li><code>filter</code> - only passes on those rows where an expression evaluates to <code>true</code>.</li> <li><code>aggregate</code> - performs windowed or non-windowed aggregations on the incoming data. Outputs updates to the aggregation as windows close.</li> <li><code>join</code> - joins a stream with a stream, or stream with a table based on matching keys. Joins can be inner or outer joins.</li> <li><code>union</code> - takes data from multiple inputs and combines them into a single stream</li> <li><code>store stream</code> - persistently stores the stream</li> <li><code>store table</code> - stores the stream as a table, i.e. later values of same key overwrite previous ones.</li> <li><code>bridge from</code> - consumes messages from an external Kafka topic (e.g. in Apache Kafka or RedPanda or another Tektite cluster)</li> <li><code>bridge to</code> - sends messages to an external Kafka topic (e.g. in Apache Kafka or RedPanda or another Tektite cluster)</li> <li><code>backfill</code> - used for back-filling a new stream from an existing stream.</li> </ul> <p>There are some operators that can only be used in queries:</p> <ul> <li><code>scan</code> - performs a range scan based on key from an existing stored table/stream</li> <li><code>get</code> - looks up a row based on key</li> <li><code>sort</code> - sorts data according to an expression list</li> </ul>"},{"location":"conceptual_model/#batches","title":"Batches","text":"<p>Data flows through operators in batches. Like a database table, a batch has columns and rows, and the column names and types are defined by a schema. An example schema for a batch might be:</p> <pre><code>customer_id: string\nage: int\nregistration_date: timestamp\nphoto: bytes\n</code></pre> <p>An operator typically has an input schema and an output schema, which can be different.</p>"},{"location":"conceptual_model/#data-types","title":"Data types","text":"<p>Columns have a datatype. We try and keep the data types as simple as we can - no need for lots of different int or string types.</p> <ul> <li><code>int</code> - a 64 bit signed integer.</li> <li><code>float</code> - a 64 bit floating point number.</li> <li><code>bool</code> - a boolean - <code>true</code> or <code>false</code></li> <li><code>decimal(precision, scale)</code> - exact decimal type, parameterised by <code>precision</code> and <code>scale</code>.</li> <li><code>string</code> - a variable length string.</li> <li><code>bytes</code> - a variable length string of bytes</li> <li><code>timestamp</code> - represents a date/time with millisecond precision</li> </ul> <p>Like a relational database, column values can be a valid value of the appropriate type or <code>null</code>.</p>"},{"location":"configuration/","title":"Server configuration reference","text":"Property Name Description Property Type Default Value processing-enabled Is processing enabled on this node? bool true level-manager-enabled Is a level manager enabled on this node? bool true node-id Unique identifier for this node. int - cluster-addresses List of addresses for intra cluster traffic list - cluster-tls-enabled Is TLS enabled for intra-cluster traffic bool false cluster-tls-key-path Path to a PEM encoded file containing the server private key string - cluster-tls-cert-path Path to a PEM encoded file containing the server certificate string - cluster-tls-client-certs-path Path to a PEM encoded file containing trusted client certificates and/or CA certificates. Only needed with TLS client authentication string - cluster-tls-client-auth Client certificate authentication mode. One of: no-client-cert, request-client-cert, require-any-client-cert,verify-client-cert-if-given, require-and-verify-client-cert string - external-level-manager-addresses List of addresses for level manager when using external level manager. list - external-level-manager-tls-enabled Is TLS enabled for connecting to external level manager? bool false external-level-manager-tls-key-path Path to a PEM encoded file containing the client private key string - external-level-manager-tls-cert-path Path to a PEM encoded file containing the client certificate string - external-level-manager-tls-client-certs-path Path to a PEM encoded file containing trusted server certificates and/or CA certificates. Only needed with TLS client authentication string - cluster-name Unique name of the cluster. string \"tektite_cluster\" registry-format Registry format for storing metadata. int 1 master-registry-record-id ID of the master registry record. string \"tektite_master\" max-registry-segment-table-entries Maximum entries in a registry segment. int 50000 level-manager-flush-interval Interval between level manager flushes. duration \"5s\" segment-cache-max-size Maximum size of the segment cache in number of segments int 100 l0-compaction-trigger Min number of tables before L0 compaction will occur int 4 l0-max-tables-before-blocking Number of L0 tables before blocking writes. int 10 l1-compaction-trigger Min number of tables before L1 compaction will occur int 4 level-multiplier Multiplier for compaction trigger between levels for compaction. int 10 compaction-poller-timeout Timeout for compaction poller. duration \"1s\" compaction-job-timeout Timeout for compaction jobs. duration \"30s\" ss-table-delete-check-period Interval for checking SSTables for deletion. duration \"2s\" ss-table-delete-delay Delay before deleting SSTables after no longer referenced. duration \"10s\" level-manager-retry-delay Delay between retries for level manager operation when unavailable duration \"250ms\" ss-table-register-retry-delay Delay between retries for registering SSTables. duration \"1s\" prefix-retention-remove-check-period Interval for checking prefix retention removal. duration \"30s\" compaction-max-ss-table-size Maximum size for an SSTable created during compaction, in bytes. string \"16777216\" table-cache-max-size-bytes Maximum size for the table cache in bytes. string \"134217728\" compaction-workers-enabled Enables compaction workers. bool false compaction-worker-count Max number of compaction workers on node. int 4 ss-table-push-retry-delay Delay between retries for pushing SSTables to object store. duration \"1s\" prefix-retention-refresh-interval Interval for refreshing prefix retentions from server. duration \"10s\" command-compaction-interval Interval between command manager compactions. duration \"5m\" cluster-manager-lock-timeout Timeout for cluster manager lock acquisition. duration \"2m\" cluster-manager-key-prefix Key prefix used by cluster manager. string \"tektite_clust_data/\" cluster-manager-listen-addresses Addresses of etcd servers list - cluster-eviction-timeout Timeout before evicting cluster member duration \"5s\" cluster-state-update-period Interval before calling into etcd with node state. duration \"2s\" etcd-call-timeout Timeout for etcd calls made by cluster manager. duration \"5s\" sequences-object-name Key of the object which stores sequences. string \"tektite_sequences\" sequences-retry-delay Delay between retries for sequence manager operations. duration \"250ms\" object-store-type Type of the object store. string \"dev\" dev-object-store-addresses Addressed of dev object store list [\"127.0.0.1:6690\"] minio-endpoint Endpoint address of Minio object store. string - minio-access-key Access key for Minio object store. string - minio-secret-key Secret key for Minio object store. string - minio-bucket-name Name of the bucket in Minio object store. string - minio-secure Flag to enable secure connection to Minio object store. bool false processor-count Number of processors. If level manager is enabled there will be one more than this. int 16 max-processor-batches-in-progress Maximum queued actions per processor before blocking. int 1000 memtable-max-size-bytes Maximum size of the mem-table in bytes. string \"16777216\" memtable-max-replace-time Maximum time before adding mem-table to the push queue. duration \"30s\" memtable-flush-queue-max-size Maximum number of mem-tables in the flush queue before blocking writes. int 10 store-write-blocked-retry-interval Interval between retries for store write operations when blocked. duration \"250ms\" table-format Data format for tables. int 1 min-replicas Minimum number of available replicas of a processor before replications are rejected. int 2 max-replicas Maximum number of replicas for a processor. int 3 min-snapshot-interval Minimum interval between creating version snapshots. duration \"200ms\" batch-flush-check-interval Interval between checking if batches can be flushed from replicas. duration \"1s\" consumer-retry-interval Interval between reconnect retries for Kafka consumers in 'bridge from' operator duration \"2s\" max-backfill-batch-size Maximum size of a batch in 'backfill' operator. int 1000 forward-resend-delay Delay before resending forwarded messages after unavailability. duration \"250ms\" query-max-batch-rows Maximum number of rows for a query result batch. int 1000 http-api-enabled Flag to enable the HTTP API server. bool - http-api-addresses List of addresses on which the HTTP API listens list - http-api-tls-key-path Path to a PEM encoded file containing the server private key string - http-api-tls-cert-path Path to a PEM encoded file containing the server certificate string - http-api-tls-client-certs-path Path to a PEM encoded file containing trusted client certificates and/or CA certificates. Only needed with TLS client authentication string - http-api-tls-client-auth Client certificate authentication mode. One of: no-client-cert, request-client-cert, require-any-client-cert,verify-client-cert-if-given, require-and-verify-client-cert string - http-api-path URI path prefix for the HTTP API server. string \"/tektite\" admin-console-enabled Flag to enable the admin console. bool false admin-console-addresses List of listen addresses for the web UI. list - admin-console-tls-key-path Path to a PEM encoded file containing the server private key string - admin-console-tls-cert-path Path to a PEM encoded file containing the server certificate string - admin-console-tls-client-certs-path Path to a PEM encoded file containing trusted client certificates and/or CA certificates. Only needed with TLS client authentication string - admin-console-tls-client-auth Client certificate authentication mode. One of: no-client-cert, request-client-cert, require-any-client-cert,verify-client-cert-if-given, require-and-verify-client-cert string - admin-console-sample-interval Interval between refreshing data for display on the web UI. duration \"5s\" kafka-server-enabled Flag to enable the Kafka server. bool false kafka-server-addresses List of addresses at which Kafka server listens. list - kafka-server-tls-key-path Path to a PEM encoded file containing the server private key string - kafka-server-tls-cert-path Path to a PEM encoded file containing the server certificate string - kafka-server-tls-client-certs-path Path to a PEM encoded file containing trusted client certificates and/or CA certificates. Only needed with TLS client authentication string - kafka-server-tls-client-auth Client certificate authentication mode. One of: no-client-cert, request-client-cert, require-any-client-cert,verify-client-cert-if-given, require-and-verify-client-cert string - kafka-use-server-timestamp If true then server time is used for timestamp for incoming messages. bool false kafka-min-session-timeout Minimum session timeout for Kafka consumers. duration \"6s\" kafka-max-session-timeout Maximum session timeout for Kafka consumers. duration \"30m\" kafka-initial-join-delay Initial delay before a new Kafka consumer can join a consumer group. duration \"3s\" kafka-new-member-join-timeout Timeout for new consumers joining Kafka consumer group. duration \"5m\" kafka-fetch-cache-max-size-bytes Maximum size of messages stored in Kafka fetch cache. string \"134217728\" life-cycle-endpoint-enabled Flag to enable life cycle \"healthcheck\" endpoint. bool false life-cycle-address Address for life cycle \"healthcheck\" endpoint listener. string - startup-endpoint-path Path for startup endpoint. string - ready-endpoint-path Path for ready endpoint. string - live-endpoint-path Path for live endpoint. string - metrics-bind Address for binding Prometheus metrics. string \"localhost:9102\" metrics-enabled Flag to enable Prometheus metrics. bool false version-completed-broadcast-period Period for broadcasting version completion. duration \"500ms\" version-manager-store-flushed-period Period between storing last flushed version to the level manager. duration \"1s\" wasm-module-instances Number of Wasm module instances per module. int 8 dd-profiler-types Comma-separated list of Datadog profiler types. string - dd-profiler-host-env-var-name Environment variable name for Datadog profiler host. string - dd-profiler-port Port for Datadog profiler. int - dd-profiler-service-name Service name for Datadog profiler. string - dd-profiler-environment-name Environment name for Datadog profiler. string - dd-profiler-version-name Version name for Datadog profiler. string - source-stats-enabled Flag to enable source statistics. bool false mem-profile-enabled Flag to enable memory profiling. bool false cpu-profile-enabled Flag to enable CPU profiling. bool false debug-server-enabled Flag to enable debug server. bool false debug-server-addresses List of addresses for debug server listener. list -"},{"location":"cookbook/","title":"Tektite Cookbook","text":""},{"location":"cookbook/#topics","title":"Topics","text":""},{"location":"cookbook/#vanilla-topic","title":"Vanilla topic","text":"<pre><code>my-topic := (topic partitions = 16);\n</code></pre>"},{"location":"cookbook/#topic-with-filter-on-json-field-in-message","title":"Topic with filter on JSON field in message","text":"<pre><code>filtered-topic :=\n    (kafka in partitions = 16) -&gt;\n    (filter by json_string(\"country\", val) == \"USA\") -&gt;\n    (kafka out);    \n</code></pre>"},{"location":"cookbook/#topic-with-filter-on-kafka-header","title":"Topic with filter on Kafka header","text":"<pre><code>filtered-topic :=\n    (kafka in partitions = 16) -&gt;\n    (filter by kafka_header(\"sender_id\", hdrs) == \"sender1234\") -&gt;\n    (kafka out);    \n</code></pre>"},{"location":"cookbook/#topic-that-transforms-body","title":"Topic that transforms body","text":"<pre><code>transformed-topic :=\n    (kafka in partitions = 16) -&gt;\n    (project key, hdrs, to_bytes(to_upper(to_string(val)))) -&gt;\n    (kafka out);\n</code></pre>"},{"location":"cookbook/#topic-that-transforms-body-with-wasm-function","title":"Topic that transforms body with WASM function","text":"<pre><code>transformed-topic :=\n    (kafka in partitions = 16) -&gt;\n    (project key, hdrs, my_mod.my_wasm_function(val)) -&gt;\n    (kafka out);\n</code></pre>"},{"location":"cookbook/#write-only-topic-to-tektite-stream","title":"Write only topic to Tektite stream","text":"<pre><code>write-only-topic :=\n    (kafka in partitions = 16) -&gt;\n    (store stream);\n</code></pre>"},{"location":"cookbook/#write-only-topic-that-sends-to-external-topic","title":"Write only topic that sends to external topic","text":"<pre><code>write-only-topic :=\n    (kafka in partitions = 16) -&gt;\n    (bridge to external-topic props = (\"bootstrap.servers\" = \"mykafka.foo.com:9092\"));\n</code></pre>"},{"location":"cookbook/#expose-existing-stream-as-read-only-topic","title":"Expose existing stream as read-only topic","text":"<pre><code>read-only-topic := existing-stream -&gt;\n    (project to_bytes(customer_id) as key, nil, to_bytes(description) as val) -&gt;\n    (kafka out);\n</code></pre>"},{"location":"cookbook/#expose-external-topic-as-read-only-tektite-topic","title":"Expose external topic as read only Tektite topic","text":"<pre><code>read-only-topic := \n    (bridge from external-topic partitions = 16\n        props = (\"bootstrap.servers\" = \"mykafka.foo.com:9092\")) -&gt;\n    (kafka out);    \n</code></pre>"},{"location":"cookbook/#expose-external-topic-as-read-only-tektite-topic-after-filtering","title":"Expose external topic as read only Tektite topic after filtering","text":"<pre><code>read-only-topic := \n    (bridge from external-topic partitions = 16\n        props = (\"bootstrap.servers\" = \"mykafka.foo.com:9092\")) -&gt;\n    (filter by json_string(\"paymment_type\", val) == \"card\") -&gt;    \n    (kafka out);    \n</code></pre>"},{"location":"cookbook/#joins","title":"Joins","text":""},{"location":"cookbook/#join-two-tektite-topics-to-create-new-read-only-topic","title":"Join two Tektite topics to create new read-only topic","text":"<pre><code>sales := (topic partitions = 16);\n\npayments := (topic partitions = 16);\n\njoined := (join sales with payments on cust_id = c_id within 10m) -&gt;\n          (project l_key, nil, concat(l_val, r_val)) -&gt;\n          (kafka out);\n</code></pre>"},{"location":"cookbook/#join-tektite-topic-with-external-topic-and-store-as-internal-stream","title":"Join Tektite topic with external-topic and store as internal stream","text":"<pre><code>sales := (bridge from sales-external partitions = 16\n          props = (\"bootstrap.servers\" = \"mykafka.foo.com:9092\"));\n\npayments := (topic partitions = 16);\n\njoined := (join sales with payments on cust_id = c_id within 10m) -&gt;\n          (store stream);\n</code></pre>"},{"location":"cookbook/#enrich-tektite-topic-with-customer-data","title":"Enrich Tektite topic with customer data","text":"<pre><code>sales := (topic partitions = 16);\n\nenriched := (join sales with cust_data on cust_id *= id) -&gt; (store stream);\n</code></pre>"},{"location":"cookbook/#repartitioning","title":"Repartitioning","text":""},{"location":"cookbook/#repartition-external-topic","title":"Repartition external topic","text":"<pre><code>repartitioner :=\n    (bridge from sales-external partitions = 16\n          props = (\"bootstrap.servers\" = \"mykafka.foo.com:9092\")) -&gt;\n    (partition by json_string(\"cust_id\", val) partitions = 100) -&gt;\n    (bridge to sales-by-customer props = (\"bootstrap.servers\" = \"mykafka.foo.com:9092\"));              \n</code></pre>"},{"location":"cookbook/#aggregations","title":"Aggregations","text":""},{"location":"cookbook/#sales-totals-by-region-last-hour-from-external-topic","title":"Sales totals by region last hour from external topic","text":"<pre><code>sales_tots :=\n    (bridge from sales-external partitions = 16\n        props = (\"bootstrap.servers\" = \"mykafka.foo.com:9092\")) -&gt;\n    (project json_string(\"country\", val) as country, json_float(\"amount\") as amount) -&gt;    \n    (partition by country partitions = 10) -&gt;\n    (aggregate count(amount) as num, sum(amount) as tot by country);\n</code></pre>"},{"location":"cookbook/#union","title":"Union","text":""},{"location":"expressions/","title":"Expressions","text":"<p>Expressions are used in Tektite projections, filters, aggregations and sorts.</p> <p>Expressions are composed of constants, column identifiers, operators and function calls.</p> <p>Expressions have a return type which is any of the Tektite data types: <code>int</code>, <code>float</code>, <code>bool</code>, <code>decimal</code>, <code>string</code>, <code>bytes</code> or <code>timestamp</code>.</p> <p>When evaluated, an expression will return a value with a type equal to the return type, or <code>null</code>.</p> <p>If any operand to an expression is null, the expression evaluates to <code>null</code>.</p>"},{"location":"expressions/#constants","title":"Constants","text":"<p>These can be </p> <ul> <li>String literals. Enclosed in double quotes, e.g. <code>\"aardvarks\"</code>. The quotes are not part of the string literal. If your string literal contains quotes, they must be escaped with backslash e.g. <code>\"\\\"hello\\\"\"</code></li> <li>Integer literals. E.g. <code>23</code>, <code>134353</code>, <code>-45</code>, <code>0</code></li> <li>Float literals. These are suffixed with <code>f</code>. E.g. <code>23.23f</code>, <code>-2.3e22f</code></li> <li>Bool literals. <code>true</code> or <code>false</code></li> </ul>"},{"location":"expressions/#column-identifiers","title":"Column identifiers","text":"<p>A column identifier references a column in the schema that the expression applies to. It's just the name of the column:</p>"},{"location":"expressions/#operators","title":"Operators","text":"<p>Tektite supports the following operators:</p> <p>Arithmetic operators</p> <ul> <li><code>+</code> - addition: supported for types <code>int</code>, <code>float</code>, <code>decimal</code>, <code>timestamp</code></li> <li><code>-</code> - subtraction: supported for types <code>int</code>, <code>float</code>, <code>decimal</code>, <code>timestamp</code></li> <li><code>*</code> - multiplication: supported for types <code>int</code>, <code>float</code>, <code>decimal</code>, <code>timestamp</code></li> <li><code>/</code> - division: supported for types <code>int</code>, <code>float</code>, <code>decimal</code>, <code>timestamp</code></li> <li><code>%</code> - modulus: supported for type <code>int</code></li> </ul> <p>Comparison operators</p> <ul> <li><code>==</code> - equality: supported for <code>int</code>, <code>float</code>, <code>bool</code>, <code>decimal</code>, <code>string</code>, <code>bytes</code>, <code>timestamp</code></li> <li><code>!=</code> - inequality: supported for <code>int</code>, <code>float</code>, <code>bool</code>, <code>decimal</code>, <code>string</code>, <code>bytes</code>, <code>timestamp</code></li> <li><code>&gt;</code>  - greater than: supported for <code>int</code>, <code>float</code>, <code>decimal</code>, <code>string</code>, <code>bytes</code>, <code>timestamp</code></li> <li><code>&gt;=</code> - greater than or equal: supported for <code>int</code>, <code>float</code>, <code>decimal</code>, <code>string</code>, <code>bytes</code>, <code>timestamp</code></li> <li><code>&lt;</code>  - less than: supported for <code>int</code>, <code>float</code>, <code>decimal</code>, <code>string</code>, <code>bytes</code>, <code>timestamp</code></li> <li><code>&lt;=</code> - less than or equal: supported for <code>int</code>, <code>float</code>, <code>decimal</code>, <code>string</code>, <code>bytes</code>, <code>timestamp</code></li> </ul> <p><code>bool</code>ean operators - operands must be of type <code>bool</code></p> <ul> <li><code>&amp;&amp;</code> - and</li> <li><code>||</code> - or</li> <li><code>!</code> - not</li> </ul>"},{"location":"expressions/#function-calls","title":"Function calls","text":"<p>Tektite comes with a library of built-in functions - and you can also write your own custom functions using WebAssembly and use them the same way as the built-in functions.</p> <p>Functions are used in expressions by writing the function name followed by left parenthesis followed by expression list followed by right parenthesis.</p>"},{"location":"expressions/#examples","title":"Examples","text":"<p>Here are some example expressions:</p> <pre><code>my-stream := parent-stream -&gt;\n    (project col3 + 10, sub_str(col7, 10 * col2, 12), col9 == \"uk\") -&gt;\n    (store stream)\n\nmy-stream := parent-stream -&gt;\n    (filter by my_wasm_func(price * 1.23f) &amp;&amp; !my_other_wasm_func(name)) -&gt;\n    (store stream)    \n\nsales_tots := sales -&gt;\n    (aggregate count(price), sum(price + adjust) by \n        to_lower(country), if(is_capital, city, \"other\"))    \n\n(scan all from sales_tots)\n    -&gt; (sort by sub_str(col2, 3, 10), to_lower(col2))     \n\n</code></pre>"},{"location":"filtering/","title":"Filters","text":"<p>Rows are filtered using the <code>filter</code> operator. In the operator definition you write <code>filter by</code> followed by a single expression. The expression is evaluated on incoming rows, and rows are passed through if the expression evaluates to <code>true</code>, otherwise they are ignored.</p> <p>Expressions are composed of constants, column identifiers, operators and functions. Any built-in function can be used, and you can also create your own functions using WebAssembly.</p> <p>For an in-depth explanation of the expression language please see the section on expressions. To learn more about creating custom WebAssembly functions please see the section on WebAssembly</p> <p>Here are some examples:</p> <p>Extract UK sales into a new stream</p> <pre><code>uk_sales := sales -&gt; (filter by country == \"UK\") -&gt; (store stream)\n</code></pre> <p>Filter using a custom WebAssembly function <code>fraud_score</code></p> <pre><code>fraudulent_tx := tx -&gt; (filter by fraud_score(name, age, price, product_id) &gt; 75) -&gt; (store stream)\n</code></pre>"},{"location":"functions/","title":"Functions reference","text":"<p>Tektite has a built-in library of functions that can be used in any expression.</p> <p>The library is very much growing, and we will add to it gradually over time as needs demand. As well as the built-in functions you can write your own functions using WebAssembly.</p>"},{"location":"functions/#function-signature-notation","title":"Function signature notation","text":"<p>When writing out a function signature in this guide we use the notation <code>&lt;name&gt;: &lt;type&gt;</code> to show the name and type of a function parameter. We also use <code>: &lt;type&gt;</code> at the end of the signature to show the return type of the function.</p> <p>Where <code>&lt;type&gt;</code> is a Tektite data type; one of <code>int</code>, <code>float</code>, <code>bool</code>, <code>decimal</code>, <code>string</code>, <code>bytes</code> or <code>timestamp</code>, or a type parameter written as an upper case letter, e.g. <code>T</code> or <code>R</code> where the type parameter refers to a Tektite type.</p> <p>If a type parameter is present, the set of concrete types that it allows will be specified like:</p> <p><code>T \u2208 {int, bool, decimal}</code></p> <p>Sometimes parameters can be optional. If so, they are placed in square brackets.</p> <p>For example:</p> <pre><code>foo(x: int, y: float, [z: bool]): string\n</code></pre> <p>Function name is <code>foo</code>. First parameter <code>x</code> is of type <code>int</code>. Second parameter <code>y</code> is of type float. Third parameter <code>z</code> is of type bool and is optional. Function returns a <code>string</code></p> <pre><code>bar(x: bool, r: R): R\nR \u2208 {int, float, bool, decimal, string, bytes, timestamp}\n</code></pre> <p>Function name is <code>bar</code>. First parameter <code>x</code> is of type <code>bool</code>. Second parameter <code>r</code> is of type <code>R</code>. Type <code>R</code> is one of <code>int</code>, <code>float</code>, <code>bool</code>, <code>decimal</code>, <code>string</code>, <code>bytes</code> or <code>timestamp</code>. Function returns type <code>R</code>.</p>"},{"location":"functions/#control-flow-functions","title":"Control flow functions","text":""},{"location":"functions/#if","title":"if","text":"<pre><code>if(e: bool, t: R, f: R): R\nR \u2208 {int, float, bool, decimal, string, bytes, timestamp}\n</code></pre> <p>Like an <code>if</code> statement.</p> <p>If <code>e</code> is <code>true</code> then return <code>t</code>, else return <code>f</code>.</p> <p>Examples:</p> <pre><code>(project if(col2 &gt; 100, \"antelopes\", \"zebras\"))\n</code></pre>"},{"location":"functions/#case","title":"case","text":"<pre><code>case(t: T, c1:T, r1:R, c2:T, r2:R, c3:T, r3:R, ..., d:R): R\n</code></pre> <p>Like a case statement. You provide the value to be tested followed by a list of (value to compare to, result to return) pairs, and a default.</p> <p>The value to be tested is <code>t</code>. It is tested against <code>c1</code>, <code>c2</code>, <code>c3</code>, etc. The first one it matches against, the corresponding return value is returned. For example, if it matches <code>c2</code>, then <code>r2</code> is returned. If it matches <code>c3</code> then <code>r3</code> is returned. If it does not match any of the <code>c</code> values, then the default value <code>d</code> is returned.</p> <p>Examples:</p> <pre><code>project(\n    case(col2, \n         17, \"USA\",\n         23, \"UK\",\n         100, \"FRANCE\",\n         \"EGYPT\")\n)\n</code></pre>"},{"location":"functions/#comparison-functions","title":"Comparison functions","text":""},{"location":"functions/#is_null","title":"is_null","text":"<pre><code>is_null(e: T): bool\nR \u2208 {int, float, bool, decimal, string, bytes, timestamp}\n</code></pre> <p>If <code>e</code> is <code>null</code> then return <code>true</code>, else return `false.</p> <p>Examples:</p> <pre><code>(filter by is_null(col2))\n</code></pre>"},{"location":"functions/#is_not_null","title":"is_not_null","text":"<pre><code>is_not_null(e: T): bool\nR \u2208 {int, float, bool, decimal, string, bytes, timestamp}\n</code></pre> <p>If <code>e</code> is not <code>null</code> then return <code>true</code>, else return `false.</p> <p>Examples:</p> <pre><code>(filter by is_not_null(col2))\n</code></pre> <p>Equivalent to:</p> <pre><code>!is_null(col2)\n</code></pre>"},{"location":"functions/#in","title":"in","text":"<pre><code>in(t: E, e1: E, e2: E, e3: E, ...): bool\n</code></pre> <p>If <code>t</code> is equal to any of <code>e1</code>, <code>e2</code>, <code>e3</code>, ... then return <code>true</code>, else return <code>false</code></p> <p>Examples:</p> <p>Returns <code>true</code> if <code>col7</code> is equal to <code>\"usa\"</code>, <code>\"uk\"</code> or <code>\"germany\"</code></p> <pre><code>(filter by in(col7, \"usa\", \"uk\", \"germany\"))\n</code></pre> <p>Returns <code>true</code> if <code>col1</code>, <code>col2</code> or <code>col3</code> are equal to <code>\"usa\"</code></p> <pre><code>(filter by in(\"usa\", col1, col2, col3))\n</code></pre>"},{"location":"functions/#decimal-functions","title":"Decimal functions","text":""},{"location":"functions/#decimal_shift","title":"decimal_shift","text":"<pre><code>decimal_shift(d: decimal, places: int, [round: bool]): decimal\n</code></pre> <p>Shifts a decimal <code>d</code> by <code>places</code> decimal places. If optional <code>round</code> is <code>true</code>, then result is rounded. Default value of <code>round</code> is <code>true</code>.</p> <p>Examples:</p> <pre><code>decimal_shift(col7, 2)\ndecimal_shift(col7, -4, false)\n</code></pre>"},{"location":"functions/#string-functions","title":"String functions","text":""},{"location":"functions/#starts_with","title":"starts_with","text":"<pre><code>starts_with(s: string, prefix: string): bool\n</code></pre> <p>Returns <code>true</code> if the string <code>s</code> starts with the prefix <code>prefix</code>, else returns <code>false</code>.</p> <p>Examples:</p> <pre><code>(filter by starts_with(name, \"smi\"))\n</code></pre>"},{"location":"functions/#ends_with","title":"ends_with","text":"<pre><code>ends_with(s: string, suffix: string): bool\n</code></pre> <p>Returns <code>true</code> if the string <code>s</code> ends with the suffix <code>suffix</code>, else returns <code>false</code>.</p> <p>Examples:</p> <pre><code>(filter by ends_with(filename, \".txt\"))\n</code></pre>"},{"location":"functions/#matches","title":"matches","text":"<pre><code>matches(s: string, re: string): bool\n</code></pre> <p>Returns true if the string <code>s</code> matches the regular expression <code>re</code>, else returns <code>false</code></p> <p>The regular expression syntax is that used by golang, and is the same general syntax as used by many other languages.</p> <p>Examples:</p> <p>Filter on <code>feed_name</code> starts with <code>\"news\"</code></p> <pre><code>(filter by matches(feed_name, \"^news\")) \n</code></pre> <p>Sort by whether <code>description</code> contains the string <code>\"apples\"</code></p> <pre><code>(filter by matches(description, \"apples\")) \n</code></pre>"},{"location":"functions/#trim","title":"trim","text":"<pre><code>trim(s: string, cutset: string): string\n</code></pre> <p>Trims any characters contained in <code>cutset</code> from the beginning and end of <code>s</code> and returns the result.</p> <p>Examples:</p> <p>Filter by <code>country</code> column == <code>\"UK\"</code> after trimming any leading and trailing whitespace (any of space character, tab character, newline or carriage return)</p> <pre><code>(filter by trim(country, \" \\t\\n\\r\") == \"UK\")\n</code></pre>"},{"location":"functions/#ltrim","title":"ltrim","text":"<pre><code>ltrim(s: string, cutset: string): string\n</code></pre> <p>Trims any characters contained in <code>cutset</code> from the beginning of<code>s</code> and returns the result.</p> <p>Examples:</p> <p>Filter by the <code>city</code> column == <code>\"Bristol\"</code> after trimming any leading whitespace (any of space character, tab character, newline or carriage return)</p> <pre><code>(filter by trim(city, \" \\t\\n\\r\") == \"Bristol\")\n</code></pre>"},{"location":"functions/#rtrim","title":"rtrim","text":"<pre><code>rtrim(s: string, cutset: string): string\n</code></pre> <p>Trims any characters contained in <code>cutset</code> from the end of<code>s</code> and returns the result.</p> <p>Examples:</p> <p>Filter by the <code>city</code> column == <code>\"Bristol\"</code> after trimming any trailing whitespace (any of space character, tab character, newline or carriage return)</p> <pre><code>(filter by trim(city, \" \\t\\n\\r\") == \"Bristol\")\n</code></pre>"},{"location":"functions/#to_lower","title":"to_lower","text":"<pre><code>to_lower(s: string): string\n</code></pre> <p>Converts <code>s</code> to lower case characters and returns the result.</p> <p>Examples:</p> <pre><code>(filter by to_lower(city) == \"manchester\")\n</code></pre>"},{"location":"functions/#to_upper","title":"to_upper","text":"<pre><code>to_upper(s: string): string\n</code></pre> <p>Converts <code>s</code> to upper case characters and returns the result.</p> <p>Examples:</p> <pre><code>(filter by to_upper(city) == \"MANCHESTER\")\n</code></pre>"},{"location":"functions/#sub_str","title":"sub_str","text":"<pre><code>sub_string(s: string, start_index: int, end_index: int): string\n</code></pre> <p>Returns a sub string of <code>s</code> from index <code>start_index</code> (inclusive) to index <code>end_index</code> (exclusive)</p> <p>Indexes start at zero.</p> <p>Examples:</p> <p>This filter would match if the <code>description</code> column contained <code>xyzapplesxyz</code>. Note that the end index is exclusive.</p> <pre><code>(filter by sub_str(description, 3, 9) == \"apples\") \n</code></pre>"},{"location":"functions/#replace","title":"replace","text":"<pre><code>replace(s: string, old: string, new: string): string\n</code></pre> <p>Replaces all instances of <code>old</code> in <code>s</code> with <code>new</code>.</p> <p>Examples:</p> <p>Replaces any occurrence of the string <code>\"my_secret_stuff\"</code> in the column <code>confidential</code> with the string <code>\"****\"</code></p> <pre><code>(project replace(confidential, \"my_secret_stuff\", \"****\") as redacted)\n</code></pre>"},{"location":"functions/#sprintf","title":"sprintf","text":"<pre><code>sprintf(pattern: string, p1: T1, p2: T2, p3: T3, ...) \n</code></pre> <p>This acts like the <code>sprintf</code> function that you find in many languages such as C, C++, golang, etc. See supported syntax</p> <p>It takes a pattern string <code>pattern</code> and a variable length list of values of any data type, and formats a string based on the pattern and the values are returns it.</p> <p>Examples:</p> <pre><code>(project sprintf(\"sensor_id:%d country:%s temperature:%.2f\",\n                 sensor_id, country, temper) as summary)\n</code></pre>"},{"location":"functions/#type-conversion-functions","title":"Type conversion functions","text":""},{"location":"functions/#to_int","title":"to_int","text":"<pre><code>to_int(v: T): int\nT \u2208 {int, float, decimal, string, timestamp}\n</code></pre> <p>Converts the value to type <code>int</code></p>"},{"location":"functions/#to_float","title":"to_float","text":"<pre><code>to_float(v: T): float\nT \u2208 {int, float, decimal, string, timestamp}\n</code></pre> <p>Converts the value to type <code>int</code></p>"},{"location":"functions/#to_string","title":"to_string","text":"<pre><code>to_string(v: T): string\nT \u2208 {int, float, bool, decimal, string, bytes, timestamp}\n</code></pre> <p>Converts the value to type <code>string</code></p>"},{"location":"functions/#to_decimal","title":"to_decimal","text":"<pre><code>to_decimal(v: T, prec: int, scale: int): decimal\nT \u2208 {int, float, decimal, string, timestamp}\n</code></pre> <p>Converts the value to type <code>decimal(prec, scale)</code></p>"},{"location":"functions/#to_bytes","title":"to_bytes","text":"<pre><code>to_bytes(v: T): bytes\nT \u2208 {string, bytes}\n</code></pre> <p>Converts the value to type <code>bytes</code></p>"},{"location":"functions/#to_timestamp","title":"to_timestamp","text":"<pre><code>to_timestamp(v: T): bytes\nT \u2208 {int, timestamp}\n</code></pre> <p>Converts the value to type <code>timestamp</code>. If the argument is of type 'int' this is interpreted as milliseconds past Unix epoch.</p>"},{"location":"functions/#date-time-functions","title":"Date / time functions","text":""},{"location":"functions/#format_date","title":"format_date","text":"<pre><code>format_date(t: timestamp, format: string): string\n</code></pre> <p>Formats the timestamp <code>t</code> into a string using the format <code>format</code>.</p> <p>The format syntax used is the golang time format</p> <p>Examples:</p> <p>Format a timestamp to a string using RFC3339 format</p> <pre><code>(project format_date(event_time, \"2006-01-02T15:04:05Z07:00\"))\n</code></pre> <p>Format in \"YYYY-MM-DD HH:MM:SS.mmm\" format:</p> <pre><code>(project format_date(event_time, \"2006-01-02 15:04:05.000\"))\n</code></pre>"},{"location":"functions/#parse_date","title":"parse_date","text":"<pre><code>parse_date(s: string, format: string): timestamp\n</code></pre> <p>Parses the string <code>s</code> using the format <code>format</code> to give a timestamp value.</p> <p>The format syntax used is the golang time format</p> <p>Examples:</p> <pre><code>(project format_date(date_str, \"2006-01-02 15:04:05.000\") as purchase_time)\n</code></pre>"},{"location":"functions/#year","title":"year","text":"<pre><code>year(d: timestamp): int\n</code></pre> <p>Extracts the year as an int from the timestamp value <code>d</code>.</p> <p>Examples:</p> <pre><code>(filter by year(event_time) == 2023)\n</code></pre>"},{"location":"functions/#month","title":"month","text":"<pre><code>month(d: timestamp): int\n</code></pre> <p>Extracts the month as an int (from 1 = January to 12 = December) from the timestamp value <code>d</code>.</p> <p>Examples:</p> <pre><code>(filter by month(event_time) == 5)\n</code></pre>"},{"location":"functions/#day","title":"day","text":"<pre><code>day(d: timestamp): int\n</code></pre> <p>Extracts the day of the month from the timestamp value <code>d</code>.</p> <p>Examples:</p> <pre><code>(filter by month(event_time) == 2 &amp;&amp; day(event_time) == 29)\n</code></pre>"},{"location":"functions/#hour","title":"hour","text":"<pre><code>hour(d: timestamp): int\n</code></pre> <p>Extracts the hour of the day (from 0 to 23) from the timestamp value <code>d</code>.</p> <p>Examples:</p> <pre><code>(filter by hour(event_time) == 7)\n</code></pre>"},{"location":"functions/#minute","title":"minute","text":"<pre><code>minute(d: timestamp): int\n</code></pre> <p>Extracts the minute of the hour (from 0 to 59) from the timestamp value <code>d</code>.</p> <p>Examples:</p> <pre><code>(filter by minute(event_time) == 23)\n</code></pre>"},{"location":"functions/#second","title":"second","text":"<pre><code>second(d: timestamp): int\n</code></pre> <p>Extracts the second of the minute (from 0 to 59) from the timestamp value <code>d</code>.</p> <p>Examples:</p> <pre><code>(filter by second(event_time) == 23)\n</code></pre>"},{"location":"functions/#millis","title":"millis","text":"<pre><code>millis(d: timestamp): int\n</code></pre> <p>Extracts the milliseconds of the second (from 0 to 999) from the timestamp value <code>d</code>.</p> <p>Examples:</p> <pre><code>(filter by millis(event_time) == 777)\n</code></pre>"},{"location":"functions/#now","title":"now","text":"<pre><code>now(): timestamp\n</code></pre> <p>Returns the current server time as a timestamp.</p> <p>Examples:</p> <pre><code>(project cust_id, name, value, now() as process_time)\n</code></pre>"},{"location":"functions/#json-functions","title":"JSON functions","text":"<p>The <code>json_xxx</code> functions, where <code>xxx</code> is <code>int</code>, <code>string</code>, etc., extract a value from a JSON object.</p> <p>They all take a <code>path</code> parameter as the first parameter. The <code>path</code> parameter traverses fields from the top object, separated by <code>.</code>, to traverse into an element of the array, you use <code>array_field_name[index]</code></p> <p>Given a Kafka message whose body is JSON, with the following format:</p> <pre><code>{\n  \"tx_id\": 12345,\n  \"cust_id\": \"cust65432\",\n  \"fruit\": [\n    \"apples\",\n    \"oranges\"\n  ],\n  \"teapot\": {\n    \"colour\": \"blue\",\n    \"age\": 23\n  }\n}\n</code></pre> <p>Path <code>tx_id</code> would retrieve <code>12345</code></p> <p>Path <code>cust_id</code> would retrieve <code>\"cust65432\"</code></p> <p>Path <code>fruit[1]</code> would retrieve <code>\"oranges\"</code></p> <p>Path <code>teapot.colour</code> would retrieve \"blue\"</p>"},{"location":"functions/#json_int","title":"json_int","text":"<pre><code>json_int(path: string, json: J): int\nJ \u2208 {string, bytes}\n</code></pre> <p>Extracts the JSON field given by <code>path</code> as an int from the JSON given by <code>json</code>. Returns null if the field does not exist.</p> <p>Example, extracts value from the body of incoming Kafka message</p> <pre><code>my-topic := (kafka in topics = 16) -&gt;\n    (project(json_int(\"tx_id\", val) as transaction_id,\n        json_int(\"teapot.age\") as teapot_age)) -&gt;\n    (store stream)\n</code></pre>"},{"location":"functions/#json_float","title":"json_float","text":"<pre><code>json_float(path: string, json: J): float\nJ \u2208 {string, bytes}\n</code></pre> <p>Extracts the JSON field given by <code>path</code> as a float from the JSON given by <code>json</code>. Returns null if the field does not exist.</p> <p>Example, extracts value from the body of incoming Kafka message</p> <pre><code>my-topic := (kafka in topics = 16) -&gt;\n    (project(json_float(\"fraud_detect.params[3].fraud_prob\", val) as fraud_prob)) -&gt;\n    (store stream)\n</code></pre>"},{"location":"functions/#json_bool","title":"json_bool","text":"<pre><code>json_bool(path: string, json: J): bool\nJ \u2208 {string, bytes}\n</code></pre> <p>Extracts the JSON field given by <code>path</code> as a bool from the JSON given by <code>json</code>. Returns null if the field does not exist.</p> <p>Example, extracts value from the body of incoming Kafka message</p> <pre><code>my-topic := (kafka in topics = 16) -&gt;\n    (project(json_bool(\"results.confirmed.card_declined\", val) as card_declined)) -&gt;\n    (store stream)\n</code></pre>"},{"location":"functions/#json_string","title":"json_string","text":"<pre><code>json_string(path: string, json: J): string\nJ \u2208 {string, bytes}\n</code></pre> <p>Extracts the JSON field given by <code>path</code> as a string from the JSON given by <code>json</code>. Returns null if the field does not exist.</p> <p>Example, extracts value from the body of incoming Kafka message</p> <pre><code>my-topic := (kafka in topics = 16) -&gt;\n    (project(json_string(\"customer.details.name\", val) as customer_name)) -&gt;\n    (store stream)\n</code></pre>"},{"location":"functions/#json_raw","title":"json_raw","text":"<pre><code>json_raw(path: string, json: J): string\nJ \u2208 {string, bytes}\n</code></pre> <p>Extracts the JSON field given by <code>path</code> as a raw string from the JSON given by <code>json</code>. Returns null if the field does not exist.</p> <p>Examples:</p> <p>Using the example JSON from the beginning of this section:</p> <pre><code>json_raw(\"teapot\", val)\n</code></pre> <p>Would return the string:</p> <pre><code>{\n  \"colour\": \"blue\",\n  \"age\": 23\n}\n</code></pre> <pre><code>json_raw(\"tx_id\", val)\n</code></pre> <p>Would return a string containing:</p> <pre><code>12345\n</code></pre> <pre><code>json_raw(\"fruit[0]\", val)\n</code></pre> <p>Would return a string containing:</p> <pre><code>\"apples\"\n</code></pre> <p>Note, the double quotes. <code>json_raw</code> returns exactly what was present in the original JSON object at that field.</p>"},{"location":"functions/#json_is_null","title":"json_is_null","text":"<pre><code>json_is_null(path: string, json: J): bool\nJ \u2208 {string, bytes}\n</code></pre> <p>Returns <code>true</code> if the JSON field given by the path has a value of JSON null, else returns <code>false</code>. Returns null if the field does not exist.</p> <p>Examples:</p> <pre><code>(project if(json_is_null(\"country\", val), \"UK\", json_string(\"country\", val)))\n</code></pre>"},{"location":"functions/#json_type","title":"json_type","text":"<pre><code>json_type(path: string, json: J): string\nJ \u2208 {string, bytes}\n</code></pre> <p>Returns the JSON type of the field given by <code>path</code> in the JSON given by <code>json</code>. Returns null if the field does not exist.</p> <p>Valid return values are:</p> <ul> <li><code>\"null\"</code>: If field has JSON type <code>null</code></li> <li><code>\"bool\"</code>: If field has JSON type <code>bool</code></li> <li><code>\"string\"</code>: If field has JSON type <code>string</code></li> <li><code>\"number\"</code>: If field has JSON type <code>number</code></li> <li><code>\"json\"</code>: If field is a JSON object or array</li> </ul>"},{"location":"functions/#kafka-helper-functions","title":"Kafka helper functions","text":""},{"location":"functions/#kafka_header","title":"kafka_header","text":"<pre><code>kafka_header(header_name: string, raw_headers: bytes): string\n</code></pre> <p>Extracts the Kafka message header with name <code>header_name</code> from the raw headers in <code>raw_headers</code>. Header is returned as a string. Returns null if header does not exist in message.</p> <p>Examples:</p> <pre><code>my-topic := (topic partitions = 16) -&gt;\n    (project to_string(key), kafka_header(\"sender_id\", hdrs) as sender_id) -&gt;\n    (store stream)\n</code></pre>"},{"location":"functions/#kafka_build_headers","title":"kafka_build_headers","text":"<pre><code>kafka_build_headers(hName1: string, hVal1: A, hName1: string, hVal1: A, hName1: string, hVal1: A, ...)\nA \u2208 {string, bytes}\n</code></pre> <p>Given a list of (header name, header value) pairs construct a raw headers block containing those headers as type bytes, in the format expected for a Kafka message.</p> <p>This is used when writing headers from the server to outgoing Kafka messages.</p> <p>Example:</p> <p>Exposing an existing stream as a consumable Kafka topic (read only)</p> <ul> <li>The key of the Kafka message will be the <code>cust_id</code> field from stream</li> <li>The message will contain a <code>server_time</code> header with current server time as value</li> <li>The message will contain a <code>country</code> header with value of country field from stream</li> <li>The message body will be the description field from the stream</li> </ul> <pre><code>my-topic := existing stream -&gt;\n    (project to_bytes(cust_id),\n        kafka_build_headers(\"server_time\", to_string(now()), \"country\", country),\n        to_bytes(description)) -&gt;\n    (kafka out)    \n</code></pre>"},{"location":"functions/#miscellaneous-functions","title":"Miscellaneous functions","text":""},{"location":"functions/#len","title":"len","text":"<pre><code>len(s: T): int\nT \u2208 {string, bytes}\n</code></pre> <p>Returns the number of bytes in <code>s</code>. Note that, in the case of a string that contains multibyte characters this will not be equal to the number of characters in the string.</p> <p>Examples:</p> <pre><code>\n(filter by len(col8) &gt; 10)\n\n(project if(len(col2) &gt; 100, \"big\", \"small\"))\n\n</code></pre>"},{"location":"functions/#concat","title":"concat","text":"<pre><code>concat(s1: T, s2: T): T\nT \u2208 {string, bytes}\n</code></pre> <p>Concatenates <code>s1</code> with <code>s2</code> and returns the result. Works with <code>string</code> or <code>bytes</code>.</p> <p>Examples:</p> <pre><code>(sort by concat(country, city))\n</code></pre>"},{"location":"functions/#bytes_slice","title":"bytes_slice","text":"<pre><code>bytes_slice(b: bytes, start: int, end: int): bytes\n</code></pre> <p>Returns a bytes value which is a slice of the byte value <code>b</code> from start index <code>start</code> (inclusive) to end index <code>end</code> (exclusive)</p> <p>Examples:</p> <pre><code>(project bytes_slice(bytes_val, 3, 10))\n</code></pre>"},{"location":"functions/#uint64_be","title":"uint64_be","text":"<pre><code>uint64_be(b: bytes): int\n</code></pre> <p>Decodes the bytes given by <code>b</code> as an unsigned 64-bit number in big-endian format.</p> <p>Example:</p> <p>If the incoming Kafka message key contains a 64-bit unsigned int in big-endian format, this will extract it as an int.</p> <pre><code>my-topic := (topic partitions = 16) -&gt;\n    (project uint64_be(key) as key, val) -&gt;\n    (store stream)\n</code></pre>"},{"location":"functions/#uint64_le","title":"uint64_le","text":"<pre><code>uint64_le(b: bytes): int\n</code></pre> <p>Decodes the bytes given by <code>b</code> as an unsigned 64-bit number in little-endian format.</p> <p>Example:</p> <p>If the incoming Kafka message key contains a 64-bit unsigned int in little-endian format, this will extract it as an int.</p> <pre><code>my-topic := (topic partitions = 16) -&gt;\n    (project uint64_le(key) as key, val) -&gt;\n    (store stream)\n</code></pre>"},{"location":"functions/#abs","title":"abs","text":"<pre><code>abs(v: T): T\nT \u2208 {int, float, decimal}\n</code></pre> <p>Returns the absolute value of <code>v</code>. That is, if <code>v</code> &gt;= 0, then return <code>v</code>, else return <code>-v</code></p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>First, please build and install Tektite.</p> <p>The rest of this guide will assume that you have the Tektite binaries on your <code>PATH</code> environment variable and the <code>TEKTITE_HOME</code> environment variable points to the directory where you cloned Tektite.</p>"},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<p>This getting started uses kcat which is a useful command line utility which allows you to directly produce and consume messages to / from any Kafka compatible server.</p> <p>You can install it on MacOS with</p> <p><code>&gt; brew install kcat</code></p>"},{"location":"getting_started/#starting-a-local-dev-server","title":"Starting a local dev server","text":"<p>For simplicity, we're going to run Tektite with a standalone configuration. This configuration is designed for fast local development and runs as a single non-clustered server with a built-in object store and cluster manager, so it doesn't need other stuff to  be running. In this configuration, data is not persistent so it's only suitable for demos and fast local iteration.</p> <p>Start Tektite:</p> <pre><code>&gt; tektited --config $TEKTITE_HOME/cfg/standalone.conf\n2024-05-07 14:37:03.647429  INFO    tektite server 0 started\n</code></pre>"},{"location":"getting_started/#start-the-cli","title":"Start the CLI","text":"<p>Now, we're going to start the Command Line Interpreter (CLI) so we can talk to Tektite. It's started with the <code>tektite</code> command:</p> <p>In a separate console:</p> <pre><code>&gt; tektite --no-verify\n</code></pre> <p>(the <code>--no-verify</code> is necessary as we're running a dev server with a self-signed certificate that won't be trusted)</p>"},{"location":"getting_started/#create-a-topic","title":"Create a topic","text":"<pre><code>tektite&gt; my-topic := (topic partitions = 1);\nOK\n</code></pre> <p>This creates a topic called <code>my-topic</code> with 1 partition.</p> <p>(We choose 1 partition here as it makes it easier to see results later on without having to enter lots of messages. But in real life you'd typically have more than one partition).</p>"},{"location":"getting_started/#produce-and-consume-some-messages","title":"Produce and consume some messages","text":"<p>In a separate console, use <code>kcat</code> to consume from the <code>my-topic</code> topic and output the message body to stdout:</p> <pre><code>&gt; kcat -q -b 127.0.0.1:8880 -G mygroup my-topic\n</code></pre> <p>In another console, we will use kcat to send messages to the topic. We will use a bash one-liner that reads text from stdin and  every time you hit enter, a message will be sent.</p> <pre><code>&gt; bash -c 'while read -r line; do echo \"$line\" | kcat -b 127.0.0.1:8880 -t my-topic -P -K: ; done'\n</code></pre> <p>Now type some text in the producer console and hit enter. You should see the messages arriving at the consumer.</p> <pre><code>&gt; bash -c 'while read -r line; do echo \"$line\" | kcat -b 127.0.0.1:8880 -t my-topic -P -K: ; done'\n\nhello\nfrom\ntektite!\n</code></pre> <pre><code>&gt; kcat -q -b 127.0.0.1:8880 -G mygroup my-topic\nhello\nfrom\ntektite!\n</code></pre> <p>We've shown that you can create topics and produce and consume from them just like any Kafka compatible server.</p> <p>Next we'll show you some features that you won't find in existing Kafka implementations.</p>"},{"location":"getting_started/#query-the-data-in-the-topic","title":"Query the data in the topic","text":"<p>With Tektite, you can also query the data in your topic. At the CLI, we'll use the <code>scan</code> operator to scan all rows in <code>my-topic</code> and then send the results to the <code>sort</code> operator which will sort it by <code>event-time</code>.</p> <p>Operators are always enclosed in parentheses. The arrow operator <code>-&gt;</code> shows that data flows from one  operator to the next.</p> <pre><code>tektite&gt; (scan all from my-topic) -&gt; (sort by event_time);\n+---------------------------------------------------------------------------------------------------------------------+\n| offset               | event_time                 | key                 | hdrs                | val                 |\n+---------------------------------------------------------------------------------------------------------------------+\n| 0                    | 2024-05-08 09:23:23.002000 | null                | .                   | hello               |\n| 0                    | 2024-05-08 09:23:24.778000 | null                | .                   | from                |\n| 0                    | 2024-05-08 09:23:28.578000 | null                | .                   | Tektite!            |\n+---------------------------------------------------------------------------------------------------------------------+\n</code></pre> <p>We can use a <code>filter</code> operator to filter out small messages:</p> <pre><code>tektite&gt; (scan all from my-topic) -&gt; (filter by len(val) &gt; 6) -&gt;\n    (sort by event_time);\n+---------------------------------------------------------------------------------------------------------------------+\n| offset               | event_time                 | key                 | hdrs                | val                 |\n+---------------------------------------------------------------------------------------------------------------------+\n| 0                    | 2024-05-08 09:23:28.578000 | null                | .                   | Tektite!            |\n+---------------------------------------------------------------------------------------------------------------------+\n1 row returned\n</code></pre>"},{"location":"getting_started/#create-a-windowed-aggregation","title":"Create a windowed aggregation","text":"<p>We're going to create a windowed aggregation that keeps track of the count and average, min and max size of messages in a one-minute window.</p> <p>Let's go back to the CLI:</p> <pre><code>tektite&gt; my-agg := my-topic -&gt;\n   (aggregate count(val), avg(len(val)), min(len(val)), max(len(val))\n    size = 1m hop = 10s);\nOK\n</code></pre> <p>This means we're going to create a new stream called <code>my-agg</code> and it's created by taking the data of <code>my-topic</code> and passing it to an <code>aggregate</code> operator.</p> <p>The aggregate operator will compute a windowed aggregation of the count of messages, the average (mean) of the message size, the minimum of the message size and the maximum of the message size in a 1-minute window. The column <code>val</code> in the output from <code>my-topic</code> contains the body of the Kafka message. The window has a hop of 10s, which means every 10 seconds a window closes and results for the last 10 seconds are output.</p> <p>Now go back to the <code>kcat</code> producer console and send a bunch more messages. Do this for more than 10 seconds. This will result in windows being closed and aggregate results made available.</p> <p>We can then look at the aggregate results on the CLI:</p> <pre><code>tektite&gt; (scan all from my-agg);\n+----------------------------------------------------------------------------------------------------------------------+\n| event_time                 | count(val)           | avg(len(val))      | min(len(val))        | max(len(val))        |\n+----------------------------------------------------------------------------------------------------------------------+\n| 2024-05-08 11:14:19.998000 | 6                    | 9.500000           | 6                    | 24                   |\n+----------------------------------------------------------------------------------------------------------------------+\n1 row returned\n</code></pre>"},{"location":"getting_started/#joining-topics","title":"Joining topics","text":"<p>Let's create another topic:</p> <pre><code>tektite&gt; other-topic := (topic partitions = 1);\nOK\n</code></pre> <p>And then we're going to create a new stream by joining <code>my-topic</code> with <code>other-topic</code> on the key of the message.</p> <p>The new stream will receive a message if messages with matching keys arrive on <code>my-topic</code> and <code>other-topic</code> within a 10-second window. The new stream is then persisted with a <code>store stream</code> operator.</p> <pre><code>tektite&gt; joined := (join my-topic with other-topic on key = key within 10s) -&gt;\n    (store stream);\nOK\n</code></pre> <p>Start a new console using <code>kcat</code> to produce messages to the topic <code>other-topic</code>.</p> <p>This time when you type messages, type them in the form <code>key:value</code> - the part before the <code>:</code> will be the key of the message and the part after will be the value.</p> <p>Type messages in both <code>kcat</code> producer consoles, some with matching keys and some with not. </p> <pre><code>&gt; bash -c 'while read -r line; do echo \"$line\" | kcat -b 127.0.0.1:8880 -t my-topic -P -K: ; done'\napples:london\noranges:brazil\nbananas:scotland\n</code></pre> <pre><code>&gt; bash -c 'while read -r line; do echo \"$line\" | kcat -b 127.0.0.1:8880 -t other-topic -P -K: ; done'\napples:france\noranges:uk\npears:uk\n</code></pre> <p>Now execute a scan in the CLI to see what's in the <code>joined</code> stream. We're going to use a <code>project</code> operator here to select out some of the columns and give them new names. The result of a join gives us columns from both inputs and we don't want them all here.</p> <pre><code>tektite&gt; (scan all from joined) -&gt; (project l_key as key, l_val, r_val);\n+--------------------------------------------------------------------------------------------------------------------+\n| key                                  | l_val                                | r_val                                |\n+--------------------------------------------------------------------------------------------------------------------+\n| apples                               | london                               | france                               |\n| oranges                              | brazil                               | uk                                   |\n+--------------------------------------------------------------------------------------------------------------------+\n2 rows returned\n</code></pre> <p>As you can see, the join has successfully matched the messages with the same keys!</p> <p>Hope you enjoyed the getting started. Please take a look at the rest of the documentation as this is just a taste of the powerful things you can do with Tektite.</p>"},{"location":"http_api/","title":"Tektite HTTP API","text":"<p>Tektite provides an HTTP API that allows Tektite statements and queries to be executed.</p> <p>It's used by the Tektite CLI, and can be used by Tektite client applications directly or via the golang Tektite client which is a wrapper around the HTTP API. The API is HTTP-2 only and requires TLS.</p> <p>Please note, this is an API that uses HTTP as the transport, it is not designed to be a RESTful API.</p>"},{"location":"http_api/#configuration","title":"Configuration","text":"<p>The addresses from which the API is served is determined by the <code>http-api-addresses</code> server configuration property. The path root for the API is determined by the <code>http-api-path</code> configuration property, and the default is <code>tektite</code>.</p>"},{"location":"http_api/#errors","title":"Errors","text":"<p>If the request succeeded HTTP response <code>200/OK</code> will be returned, otherwise a different HTTP response code will be returned and the response body will contain an error message.</p>"},{"location":"http_api/#executing-statements","title":"Executing statements","text":"<p>To execute a statement you send a <code>POST</code> request to the path <code>tektite/statement</code>. The body of the request contains the statement to execute.</p> <p>For example:</p> <pre><code>POST mytektite.foo.com:7770/tektite/statement\nmy-topic := (topic partitions = 16)\n</code></pre> <pre><code>POST mytektite.foo.com:7770/tektite/statement\ndelete(my-topic)\n</code></pre>"},{"location":"http_api/#executing-queries","title":"Executing queries","text":"<p>To execute a query you send a <code>POST</code> request to the path <code>tektite/query</code>. The body of the request contains the query to execute.</p> <p>For example:</p> <pre><code>POST mytektite.foo.com:7770/tektite/query\n(scan all from sales_figures) -&gt; (sort by country, city)\n</code></pre> <pre><code>POST mytektite.foo.com:7770/tektite/query\n(get \"customer1234\" from cust_sales) -&gt; (project cust_name, city, sales_total)\n</code></pre> <p>By default, query results are returned in the response in JSON lines format. Each row of the result is received as a JSON array, and lines are separated with newline (<code>\\n</code>)</p> <pre><code>[\"dave smith\", \"london\", 123.67]\n[\"susan hill\", \"birmingham\", 56.43]\n[\"andrew wilson\", \"manchester\", 12.99]\n</code></pre> <p>If you add the query parameter <code>col_headers</code> with the value <code>true</code>, then you will also receive column names and column types as the first two lines received:</p> <pre><code>[\"cust_name\", \"city\", \"sales_total\"]\n[\"string\", \"string\", \"float\"]\n[\"dave smith\", \"london\", 123.67]\n[\"susan hill\", \"birmingham\", 56.43]\n[\"andrew wilson\", \"manchester\", 12.99]\n</code></pre> <p>Any <code>decimal</code> values in the result will be returned as JSON strings so as not to lose any precision.</p> <p>You can also ask to receive the results encoded in Apache Arrow format if you prefer. To do this add an <code>accept</code> header in the HTTP request with value <code>x-tektite-arrow</code></p>"},{"location":"http_api/#prepared-queries","title":"Prepared queries","text":"<p>Tektite supports prepared queries. These are the same as prepared statements in other databases - we call them prepared queries as we only support preparing for queries not statements in general.</p> <p>A prepared query is parsed and the query state is set up before execution. At execution time just the parameters of the query (if any) are passed to the server in order to execute it. This has less overhead than parsing it and constructing the query state on each invocation of the query, if the query is executed many times.</p> <p>You prepare a query by executing a <code>prepare</code> statement:</p> <pre><code>POST mytektite.foo.com:7770/tektite/statement\nprepare my_query := (scan $name_start:string to $name_end:string) -&gt;\n    (filter by country == $country:string)\n</code></pre> <p>The query can have zero or more parameters, these are denoted by <code>$&lt;param_name&gt;:&lt;param_type&gt;</code>, e.g. <code>$name_start:string</code> is  a parameter called <code>name_start</code> with a type of <code>string</code>.</p> <p>Parameters can be of any Tektite data types and can appear in the query anywhere a column identifier is legal.</p> <p>To execute a prepared query, you send a <code>POST</code> request to the <code>execute</code> endpoint, and the body of the request must contain a JSON object with the name of the query to execute in the <code>QueryName</code> field and the prepared query arguments to execute it as a JSON array in the <code>Args</code> field.</p> <pre><code>POST POST mytektite.foo.com:7770/tektite/execute\n{\"QueryName\": \"my_query\", \"Args\": [\"fox\", \"smith\", \"UK\"]}\n</code></pre> <p>Tektite will attempt to convert arguments of a particular JSON type to the corresponding Tektite type in a sensible way.</p> <ul> <li>Tektite type <code>int</code>: will convert from JSON types <code>number</code>, <code>string</code>.</li> <li>Tektite type <code>float</code>: will convert from JSON types <code>number</code>, <code>string</code>.</li> <li>Tektite type <code>bool</code>: will convert from JSON types <code>bool</code>, <code>string</code> (<code>\"true\"</code>/<code>\"TRUE\"</code> and <code>\"false\"</code>/<code>\"FALSE\"</code>).</li> <li>Tektite type <code>decimal(p,s)</code>: will convert from JSON types <code>string</code>, <code>number</code>.</li> <li>Tektite type <code>string</code>: will convert from JSON types <code>string</code>, <code>number</code>, <code>bool</code> (<code>\"true\"</code>, <code>\"false\"</code>).</li> <li>Tektite type <code>bytes</code>: must be passed as a base64 encoded JSON <code>string</code>.</li> <li>Tektite type <code>timestamp</code>: will convert from a JSON <code>number</code> that represents number of milliseconds from Unix Epoch.</li> <li>For a <code>null</code> Tektite argument value, a JSON <code>null</code> should be passed as the argument.</li> </ul>"},{"location":"http_api/#registering-and-unregistering-wasm-modules","title":"Registering and unregistering WASM modules","text":"<p>You use the HTTP API to register / unregister WASM modules:</p> <p>To register a module, send a <code>POST</code> request to the path <code>tektite/wasm-register</code>. The body of the request contains a JSON object with the JSON metadata for the module in a field <code>MetaData</code> and the module bytes as a base 64 encoded string in a field <code>ModuleData</code>:</p> <p>For example:</p> <pre><code>POST mytektite.foo.com:7770/tektite/wasm-register\n{ \n   \"MetaData\": {\n       \"name\": \"my-wasm-mod\",\n        \"functions\": {\n            \"foo\": {\n                \"paramTypes\": [\"string\"],\n                \"returnType\": \"int\"\n            }\n       }\n   },\n   \"ModuleData\": \"&lt;base64 encoded module bytes&gt;\"\n}\n</code></pre> <p>To unregister a module, send a <code>POST</code> request to the path <code>tektite/wasm-unregister</code>. The body of the request contains the name of the module to unregister</p> <pre><code>POST mytektite.foo.com:7770/tektite/wasm-unregister\nmy-wasm-mod\n</code></pre>"},{"location":"installation/","title":"Build and Install","text":"<p>Until we reach version 1.0 you should build Tektite yourself from the source:</p> <pre><code>git clone https://github.com/spirit-labs/tektite.git\ncd tektite\ngo build -o bin ./...\n</code></pre> <p>This will create binaries in the <code>bin</code> directory</p> <p>Assuming the directory you cloned Tektite into is <code>$TEKTITE_HOME</code>, then you can add the binary directory to your path with (on *nix):</p> <p><code>export PATH=$PATH:$TEKTITE_HOME/bin</code></p>"},{"location":"joins/","title":"Joins","text":"<p>Tektite allows you to join streams with other streams to create new streams, or to join streams with table to create new streams.</p> <p>The streams can be any Tektite stream, so you can join two Tektite topics, a Tektite topic with an imported external topic, two external topics (e.g. an Apache Kafka topic with a RedPanda topic), or any other stream with any other stream!</p>"},{"location":"joins/#stream-stream-joins","title":"Stream-stream joins","text":"<p>A stream-stream join, joins a stream with another stream to create a new stream. Rows are joined by matching one or more key columns on the incoming streams and specifying a window.</p> <p>For example, the following join creates an output row when there's a sale and a payment for the same transaction id in a window of 5 minutes.</p> <pre><code>matched_sales :=\n    (join sales with payments on tx_id=transaction_id within = 5m) -&gt;\n    (store stream);\n</code></pre> <p>In the above the left stream is <code>sales</code> and has a key column <code>tx_id</code> which contains the transaction id. The right stream is <code>payments</code> and contains a column <code>transaction_id</code> which contains the transaction id.</p> <p>A stream-stream join must have a <code>within</code> argument that specifies the size of the window. The window size is a duration like <code>5m</code>, <code>12h</code> or <code>10s</code>.</p> <p>A join creates output columns for each input column of the left and right streams. Columns that come from the left stream are prefixed with <code>l_</code> and columns that come from the right stream are prefixed with <code>r_</code>.</p> <p>If sales has the following schema:</p> <pre><code>event_time: timestamp\ntx_id: int\nproduct_id: string\nprice: decimal(10, 2)\ncustomer_id: string\n</code></pre> <p>And payments has the following schema:</p> <pre><code>event_time: timestamp\ntransaction_id: int\npayment_type: string\npayment_provider: string\n</code></pre> <p>Then the output of the join will have the following schema:</p> <pre><code>event_time: timestamp\nl_event_time: timestamp\nl_tx_id: int\nl_product_id: string\nl_price: decimal(10, 2)\nl_customer_id: string\nr_event_time: timestamp\nr_payment_type: string\nr_payment_provider: string\n</code></pre> <p>Note that the join columns (<code>l_tx_id</code>) is only included once as it's the same for left and right.</p> <p>Often you will only want a subset of the columns or want to change their names or order. A <code>project</code> operator is used to do that:</p> <pre><code>matched_sales :=\n    (join sales with payments on tx_id = transaction_id within = 5m) -&gt;\n    (project l_tx_id as tx_id,\n             l_customer_id as cust_id,\n             l_price as price, \n             r_payment_type as payment_type) -&gt;\n    (store stream);\n</code></pre> <p>You can also join on multiple columns. In this case the join columns are separated by a comma.</p> <pre><code>matched_sales :=\n    (join sales with payments\n        on tx_id = transaction_id, country = country_id\n        within = 5m) -&gt;\n    (project l_tx_id as tx_id,\n             l_customer_id as cust_id,\n             l_price as price, \n             r_payment_type as payment_type) -&gt;\n    (store stream);\n</code></pre>"},{"location":"joins/#stream-table-joins","title":"Stream-table joins","text":"<p>Tables are persisted by <code>aggregate</code> and <code>to table</code> operators. They differ from stream data in that they have a key so later rows with the same key overwrite earlier rows, and they have no <code>offset</code> column.</p> <p>With Tektite, you can join a stream with a table. This is often used to 'enrich' stream data with value looked up from the table. The stream-table join can be an inner join, or an outer join.</p> <p>With an inner join, a row is output from the join if the incoming stream data finds a match in the table.</p> <p>With an outer join a row is output from the join whether or not a match is found in the table. If no match is found the columns in the output that come from the table will be <code>null</code>.</p> <p>Let's say we have a table <code>cust_data</code> which contains latest customer data.</p> <pre><code>cust_data := cust_updates_stream -&gt; (to table key = \"cust_id\")\n</code></pre> <p>And a stream topic sales which contains sales events:</p> <pre><code>sales := (topic partitions = 16)\n</code></pre> <p>We can create a new stream that enriches the sales stream with customer data. We will use a projection to extract the columns of interest:</p> <pre><code>enriched_sales := \n    (join sales with cust_data on c_id = customer_id) -&gt;\n    (project l_c_id as customer_id,\n             r_cust_name as customer_name\n             r_cust_address as customer_address\n             l_tx_id as tx_id,\n             l_amount as amount,\n             l_price as price) -&gt;\n    (store stream)         \n</code></pre> <p>In the above we define the join columns with a single <code>=</code> (<code>c_id = customer_id</code>). This means it's an inner join and a row will only be output if a matching customer row is found for an incoming sale.</p> <p>If we want a row to be output irrespective of whether a matching customer row was found we use <code>=*</code> to define the join  columns:</p> <pre><code>enriched_sales := \n    (join sales with cust_data on c_id *= customer_id) -&gt;\n    (project l_c_id as customer_id,\n             r_cust_name as customer_name\n             r_cust_address as customer_address\n             l_tx_id as tx_id,\n             l_amount as amount,\n             l_price as price) -&gt;\n    (store stream)         \n</code></pre> <p>This means it's an outer join. The <code>*</code> must be on the stream side of the join.</p>"},{"location":"object_stores/","title":"Object stores","text":"<p>Tektite uses an object store such as MinIO or Amazon S3 for durably storing data. Object stores are great for reliably storing huge amounts of data, but usually work best when data is stored and retrieved in large chunks. Also, operations, especially write operations, can also have high latency as they may have to touch replicas which might be in different availability zones for reliability.</p> <p>This makes object stores a poor choice as a direct data store for an event streaming / processing platform where we often need to make many small reads and writes, and we want low latency for serving data quickly to consumers.</p> <p>Tektite implements a distributed level based log structured merge tree, where the data is maintained as SSTables in multiple levels  of the tree. The SSTables are persisted in the object store and 'hot' tables are cached in the Tektite cluster for fast access. SSTables are asynchronously flushed to the object store.</p>"},{"location":"object_stores/#minio-object-store-client","title":"MinIO object store client","text":"<p>MinIO is an easy-to-use object store that can scale to very large number of objects, but also be easily deployed as single server on your laptop during development.</p> <p>Tektite uses the MinIO golang client to access an object store. MinIO is Amazon S3 compatible so this client also works when your object store is S3.</p>"},{"location":"object_stores/#setting-up-a-minio-cluster","title":"Setting up a MinIO cluster","text":"<p>Please see the MinIO docs for how to setup a MinIO cluster</p>"},{"location":"object_stores/#using-minio-for-development","title":"Using MinIO for development","text":"<p>Docs to install MinIO for MacOS are here </p> <p>In summary:</p> <pre><code>brew install minio/stable/minio\n</code></pre>"},{"location":"object_stores/#starting-minio","title":"Starting MinIO","text":"<p>Start it with</p> <pre><code>minio server ~/tektite-data\n</code></pre> <p>This would store the MinIO data in the <code>~/tektite-data</code> directory</p>"},{"location":"object_stores/#minio-console","title":"MinIO console","text":"<p>This is at <code>http://localhost:9000</code></p> <p>Default username/password is <code>minioadmin/minioadmin</code></p> <p>Here you can view and create/delete buckets and create keys.</p>"},{"location":"partitions/","title":"Partitioning data","text":""},{"location":"partitions/#processors","title":"Processors","text":"<p>Tektite processes data using processors. A Tektite server has a fixed number of processors, typically chosen to correspond to the number of available cores on the machine.</p> <p>Essentially, a processor is an event loop, which loops around consuming submitted tasks, executing them and then processing the next task when it's available. Internally, each processor uses a single goroutine to execute all work.</p>"},{"location":"partitions/#partitions","title":"Partitions","text":"<p>All Tektite streams are partitioned. The number of partitions is determined by the <code>partitions</code> parameter on the most recent upstream <code>bridge from</code>, <code>topic</code>, <code>kafka in</code> or <code>partition by</code> operator.</p> <p>Tektite pins each partition with a particular processor. Any work for a particular partition will always be processed by the same processor.</p> <p>Partitioning a stream into many partitions therefore allows us to parallelise processing of data on that stream.</p>"},{"location":"partitions/#the-partition-by-operator","title":"The <code>partition by</code> operator","text":"<p>The <code>partition by</code> operator re-partitions the stream with the specified key and number of partitions. Why would you need  to re-partition? Let's consider some examples:</p>"},{"location":"partitions/#re-partitioning-for-an-aggregate-or-store-table","title":"Re-partitioning for an <code>aggregate</code> or <code>store table</code>","text":"<p>Let's say we have a Kafka topic which has 20 partitions.</p> <pre><code>sales := (topic partitions = 20)\n</code></pre> <p>It's the Kafka client that chooses the partition for the message, not Tektite. Let's say the Kafka client used a random policy.</p> <p>We wish to create a windowed aggregation calculating sales totals by <code>country</code>. As the sales topic is not partitioned on the  <code>country</code> column, messages for a particular country could be in any partition.</p> <p>When calculating an aggregation, the aggregation is maintained per partition, so for a correct value, all messages for a particular country must be aggregated on the same partition.</p> <p>To ensure this, we re-partition the stream on the <code>country</code> field:</p> <pre><code>sales-tots := sales -&gt;\n    (project json_string(\"country\", val) as country,\n        to_decimal(json_string(\"value\", val), 10, 2) as value) -&gt;\n    (partition by country partitions = 10) -&gt;\n    (aggregate count(value), sum(value) by country size = 1h hop = 10m)\n</code></pre> <p>Similarly, when using a <code>store table</code> operator, you will need to partition by the key of the table before the <code>store table</code> operator.</p> <p>The partition by operator can also partition by multiple key columns. You would commonly partition by multiple columns when you are grouping by multiple columns in an aggregation:</p> <pre><code>sales-tots := sales -&gt;\n    (project json_string(\"country\", val) as country,\n             json_string(\"city\", val) as city,\n             to_decimal(json_string(\"value\", val), 10, 2) as value) -&gt;\n    (partition by country, city partitions = 10) -&gt;\n    (aggregate count(value), sum(value) by country, city size = 1h hop = 10m)\n</code></pre>"},{"location":"partitions/#partiton-by-const","title":"Partiton by const","text":"<p>Sometimes you want all rows to go to a single partition. This would be the case where you have an aggregatioon with no group by expressions - you want to maintain overall aggregations, or you want to just maintain the latest value seen in a <code>store table</code> operator.</p> <p>In these case you can use the special value <code>const</code> when defining the partition key.</p> <p>For example:</p> <pre><code>overall-sales-tots := sales -&gt;\n    (project json_string(\"country\", val) as country,\n             json_string(\"city\", val) as city,\n             to_decimal(json_string(\"value\", val), 10, 2) as value) -&gt;\n    (partition by const partitions = 1) -&gt;\n    (aggregate count(value), sum(value) size = 1h hop = 10m)\n</code></pre> <p>Or for a table:</p> <pre><code>last-sale := sales -&gt;\n  (partition by const partitions = 1)\n  (store table)\n</code></pre>"},{"location":"partitions/#re-partitioning-for-a-bridge-to","title":"Re-partitioning for a <code>bridge to</code>","text":"<p>If you are bridging from a Tektite stream to an external Kafka topic and the external topic has a different number of partitions then you will need to re-partition the stream first:</p> <pre><code>out-stream := sales -&gt;\n   (partition by customer_id partitions = 50) -&gt;\n   (bridge to external-sales\n       props = (\"bootstrap.servers\" = \"mykafka1.foo.com:9092\"))\n</code></pre>"},{"location":"partitions/#re-partitioning-an-existing-topic","title":"Re-partitioning an existing topic","text":"<p>Take an existing Tektite topic, repartition it with a different key and number of partitions and expose the repartitioned data as a new (read only) topic:</p> <pre><code>repartitioned := my-topic -&gt;\n    (partition by product_id partitions = 100) -&gt;\n    (kafka out)\n</code></pre>"},{"location":"partitions/#re-partitioning-an-external-topic","title":"Re-partitioning an external topic","text":"<p>Bridge an external topic into Tektite, filter out some data, repartition it and send it to another external topic.</p> <pre><code>repartition-stream :=\n    (bridge from sales-by-customer partitions = 16\n        props = (\"bootstrap.servers\" = \"mykafka1.foo.com:9092\")) -&gt;\n    (filter by json_string(\"country\", val) != \"USA\") -&gt;\n    (partition by product_id partitions = 50) -&gt;\n    (bridge to sales-by-product\n        props = (\"bootstrap.servers\" = \"mykafka1.foo.com:9092\"))\n</code></pre>"},{"location":"persistence/","title":"Persistence","text":""},{"location":"persistence/#persistent-streams","title":"Persistent streams","text":"<p>Let's take an example of creating a new stream by filtering an existing stream. This one takes the <code>sales</code> stream and only lets UK sales through:</p> <pre><code>my-stream := sales -&gt; (filter by by country = \"UK\")\n</code></pre> <p>This is a perfectly valid new stream, and can have child streams, but by itself it is not persistent.</p> <p>If you want to make a stream persistent, such that the data is durably stored, and it can be queried, then you use a <code>store stream</code> operator.</p> <pre><code>my-stream := sales -&gt;\n    (filter by by country = \"UK\") --&gt;\n    (store stream)\n</code></pre> <p>A persistent stream is given an extra column <code>offset</code>. This is an automatically generated monotonically increasing sequence per partition (there can be gaps in the sequence, but it always increases)</p> <p>The operators <code>topic</code> and <code>kafka out</code> have a \"built-in\" store stream, so you don't need to add one yourself. So the following streams are also persistent, i.e. they are queryable:</p> <pre><code>my-topic := (topic partitions=16)\n\nexposed-sales := sales -&gt; (kafka out)\n</code></pre> <p>The persisted data for a stream is never overwritten, the <code>offset</code> column always increases.</p>"},{"location":"persistence/#tables","title":"Tables","text":"<p>The other way that operators can persist queryable data is as tables.</p> <p>A table differs from a persistent stream in that it has a key. There can be multiple columns in the key, and the columns can be of any valid Tektite data type.</p> <p>Entries for the same key are updated as new data arrives with the same key.</p> <p>The operators that persist data as tables are the <code>aggregate</code> and <code>store table</code> operators.</p> <p>Here's an example aggregate:</p> <pre><code>my-agg := sales -&gt;\n    (aggregate count(amount), max(amount) by country, city)\n</code></pre> <p>This would store aggregate results in a table with key made of columns <code>country</code> and <code>city</code>.</p> <p>You can also use a <code>store table</code> operator to persist stream data with a key.</p> <p>For example, let's say we have a stream which contains IoT sensor readings and has the schema:</p> <pre><code>event_time: timestamp\nsensor_id: string\ntemperature: float\ncountry: string\narea: string\n</code></pre> <p>We could use a <code>store table</code> operator to create a new stream which takes the stream of sensor readings and persists the latest  reading for a particular sensor. This table would be queryable.</p> <pre><code>latest_sensor_readings := sensor_readings -&gt;\n    (store table by sensor_id)\n</code></pre> <p>The <code>store table</code> operator takes a list of columns.  The syntax is <code>store table by &lt;column list&gt;</code>. The column list defines which columns form the key of the table.</p> <p>The table can then be queried, e.g. to list all latest UK sensor readings:</p> <pre><code>(scan all from latest_sensor_readings) -&gt; (filter by country = \"UK\")\n</code></pre> <p>And to lookup latest reading for a specific sensor (key lookup):</p> <pre><code>(get \"sensor34567\" from latest_sensor_readings)\n</code></pre> <p>The column list can be omitted altogether in a <code>store table</code>. In this case only a single row per partition will be retained. This is often used in conjunction with a <code>partition by const</code>, to store the last received value:</p> <pre><code>overall_latest_sensor_reading := sensor_readings -&gt;\n    (partition by const partitions = 1) -&gt;\n    (store table)\n</code></pre>"},{"location":"projections/","title":"Transforming data with projections","text":"<p>A projection evaluates a list of expressions on the incoming rows in order to produce output rows. There's one expression for each output column. Projections are implemented using the <code>project</code> operator.</p> <p>By evaluating expressions to create new rows, data is transformed from one form to another.</p> <p>Expressions are composed of constants, column identifiers, operators and functions. Any built-in function can be used, and you can also create your own functions using WebAssembly.</p> <p>For an in-depth explanation of the expression language please see the section on expressions. To learn more about creating custom WebAssembly  functions please see the section on WebAssembly</p> <p>To change the name of the column, use the <code>as</code> operator. Otherwise, the name of the expression will be used for the new column name.</p> <p>Here are some examples:</p> <p>Extracting fields from an incoming JSON object:</p> <pre><code>cust_updates := (bridge from ext_cust_updates ...) -&gt;\n    (project to_string(key) as cust_id,\n             json_string(\"name\", val) as cust_name,\n             json_int(\"age\", val) as cust_age,\n             json_string(\"address\", val) as cust_address) -&gt;\n    (store stream)          \n</code></pre> <p>Just changing the order of the columns / changing column names:</p> <pre><code>cust_updates2 := cust_updates -&gt;\n    (project cust_name, cust_age as cage, cust_id, cust_address) -&gt;\n    (store stream)    \n</code></pre> <p>Transforming data using a custom WebAssembly function:</p> <pre><code>adjusted_sales := sales -&gt;\n    (project id, my_wasm_func(id, amount, price, event_time)) -&gt;\n    (store stream)\n</code></pre> <p>Projections can also be used in queries, for example:</p> <p>Omit all columns in the results apart from <code>cust_id</code> and <code>name</code>:</p> <pre><code>(scan all from cust_data) -&gt; (project cust_id, name)\n</code></pre> <p>Get a specific customer and upper case the name column and rename it</p> <pre><code>(get \"cust1234\" from cust_data) -&gt; (project cust_id, to_upper(name) as uname)\n</code></pre>"},{"location":"queries/","title":"Queries","text":"<p>Any data in a persistent stream or table can be queried.</p> <p>Tektite implements queries as transient streams that only exist as long as the query. Raw data from the query source is fed into one end of the stream, and results are obtained from the other end.</p> <p>Tektite two ways of reading data into the query stream - key lookups and range scans.</p>"},{"location":"queries/#key-lookups","title":"Key lookups","text":"<p>A key lookup retrieves all matching rows given a key value.</p> <p>For example, let's say we have a table that contains customer information and has a key <code>customer_id</code></p> <pre><code>cust_info := cust_stream -&gt;\n    (partition by customer_id partitions = 16) -&gt;\n    (store table by customer_id)\n</code></pre> <p>And we want to look up a specific customer with <code>customer_id</code> = <code>cust54323</code>.</p> <p>We use a <code>get</code> operator to do the lookup:</p> <pre><code>tektite&gt; (get \"cust54323\" from cust_info);\n+---------------------------------------------------------------------------------------------------------------------+\n| event_time                 | customer_id         | name                | age                  | city                |\n+---------------------------------------------------------------------------------------------------------------------+\n| 2024-05-15 09:10:07.375000 | cust54323           | joe bloggs          | 43                   | miami               |\n+---------------------------------------------------------------------------------------------------------------------+\n1 row returned\n</code></pre> <p>We can also look up in tables with composite keys. Let's say we have an aggregation with a composite key of <code>[country, city]</code></p> <pre><code>sales_by_country_city := sales -&gt;\n   (partition by country partitions=16) -&gt;\n   (aggregate min(amount), max(amount), count(amount), sum(amount)\n       by country, city);\n</code></pre> <p>We can use the <code>get</code> operator to look up a single row by specifying values for both <code>country</code> and <code>city</code>:</p> <pre><code>tektite&gt; (get \"usa\", \"austin\" from sales_by_country_city);\n+--------------------------------------------------------------------------------------------------------------------+\n| event_time                 | country    | city       | min(amou.. | max(amou.. | count(amount)        | sum(amou.. |\n+--------------------------------------------------------------------------------------------------------------------+\n| 2024-05-15 09:18:25.576000 | usa        | austin     | 23.23      | 23.23      | 1                    | 23.23      |\n+--------------------------------------------------------------------------------------------------------------------+\n1 row returned\n</code></pre> <p>We don't need to specify all the values for the key, we can omit ones on the right. This lets us look up all rows for a particular country:</p> <pre><code>tektite&gt; (get \"usa\" from sales_by_country_city);\n+--------------------------------------------------------------------------------------------------------------------+\n| event_time                 | country    | city       | min(amou.. | max(amou.. | count(amount)        | sum(amou.. |\n+--------------------------------------------------------------------------------------------------------------------+\n| 2024-05-15 09:18:25.576000 | usa        | austin     | 23.23      | 23.23      | 1                    | 23.23      |\n| 2024-05-15 09:18:25.957000 | usa        | miami      | 56.23      | 56.23      | 1                    | 56.23      |\n+--------------------------------------------------------------------------------------------------------------------+\n2 rows returned\n</code></pre> <p>You can use a <code>project</code> operator to transform the data received:</p> <pre><code>tektite&gt; (get \"usa\" from sales_by_country_city) -&gt;\n    (project country, to_upper(city) as ucity);\n+---------------------------------------------------------------------------------------------------------------------+\n| country                                                  | ucity                                                    |\n+---------------------------------------------------------------------------------------------------------------------+\n| usa                                                      | MIAMI                                                    |\n| usa                                                      | AUSTIN                                                   |\n+---------------------------------------------------------------------------------------------------------------------+\n</code></pre> <p>You might want to sort the results if you are returning more than one row:</p> <pre><code>tektite&gt; (get \"usa\" from sales_by_country_city) -&gt;\n    (project country, to_upper(city) as ucity) -&gt;\n    (sort by ucity);\n+---------------------------------------------------------------------------------------------------------------------+\n| country                                                  | ucity                                                    |\n+---------------------------------------------------------------------------------------------------------------------+\n| usa                                                      | AUSTIN                                                   |\n| usa                                                      | MIAMI                                                    |\n+---------------------------------------------------------------------------------------------------------------------+\n2 rows returned\n</code></pre>"},{"location":"queries/#table-scans","title":"Table scans","text":"<p>Sometimes you may want to scan all the data in a table. You do this with the <code>scan all</code> operator:</p> <pre><code>tektite&gt; (scan all from sales_by_country_city);\n+--------------------------------------------------------------------------------------------------------------------+\n| event_time                 | country    | city       | min(amou.. | max(amou.. | count(amount)        | sum(amou.. |\n+--------------------------------------------------------------------------------------------------------------------+\n| 2024-05-15 09:18:25.535000 | uk         | manchester | 56.23      | 56.23      | 1                    | 56.23      |\n| 2024-05-15 09:18:25.576000 | usa        | austin     | 23.23      | 23.23      | 1                    | 23.23      |\n| 2024-05-15 09:18:25.957000 | usa        | miami      | 56.23      | 56.23      | 1                    | 56.23      |\n| 2024-05-15 09:18:25.557000 | uk         | london     | 76.23      | 85.23      | 2                    | 161.46     |\n+--------------------------------------------------------------------------------------------------------------------+\n4 rows returned\n</code></pre> <p>As with any query you can use <code>filter</code>, <code>project</code> or <code>sort</code> in the query stream:</p> <pre><code>tektite&gt; (scan all from sales_by_country_city) -&gt;\n    (filter by city != \"manchester\") -&gt;\n    (project country, to_upper(city) as ucity) -&gt;\n    (sort by ucity);\n+---------------------------------------------------------------------------------------------------------------------+\n| country                                                  | ucity                                                    |\n+---------------------------------------------------------------------------------------------------------------------+\n| usa                                                      | AUSTIN                                                   |\n| uk                                                       | LONDON                                                   |\n| usa                                                      | MIAMI                                                    |\n+---------------------------------------------------------------------------------------------------------------------+\n3 rows returned\n</code></pre>"},{"location":"queries/#range-scans","title":"Range scans","text":"<p>A range scan matches all key values within a range.</p> <p>The key of this table is <code>[country, city]</code> so the following scan will match any countries with a country that is  lexographically greater than or equal to <code>f</code> and less than <code>h</code></p> <pre><code>tektite&gt; (scan \"f\" to \"h\" from sales_by_country_city) -&gt; (sort by country);\n+--------------------------------------------------------------------------------------------------------------------+\n| event_time                 | country    | city       | min(amou.. | max(amou.. | count(amount)        | sum(amou.. |\n+--------------------------------------------------------------------------------------------------------------------+\n| 2024-05-15 10:27:24.676000 | france     | paris      | 23.23      | 23.23      | 1                    | 23.23      |\n| 2024-05-15 10:27:24.699000 | france     | lyon       | 56.23      | 56.23      | 1                    | 56.23      |\n| 2024-05-15 10:27:24.720000 | germany    | berlin     | 23.23      | 23.23      | 1                    | 23.23      |\n| 2024-05-15 10:27:25.164000 | germany    | hamburg    | 56.23      | 56.23      | 1                    | 56.23      |\n+--------------------------------------------------------------------------------------------------------------------+\n4 rows returned\n</code></pre> <p>The range can include all components of a composite key, so the following query will just return the row for <code>lyon</code></p> <pre><code>tektite&gt; (scan \"france\", \"lyon\" to \"france\", \"m\" from\n            sales_by_country_city) -&gt;\n         (sort by country);\n+--------------------------------------------------------------------------------------------------------------------+\n| event_time                 | country    | city       | min(amou.. | max(amou.. | count(amount)        | sum(amou.. |\n+--------------------------------------------------------------------------------------------------------------------+\n| 2024-05-15 10:27:24.699000 | france     | lyon       | 56.23      | 56.23      | 1                    | 56.23      |\n+--------------------------------------------------------------------------------------------------------------------+\n1 row returned\n</code></pre> <p>By default, the lower bound of the range is inclusive and the upper bound is exclusive. So the following query also doesn't return the row for <code>paris</code>.</p> <pre><code>tektite&gt; (scan \"france\", \"lyon\" to \"france\", \"paris\" from\n             sales_by_country_city) -&gt;\n         (sort by country);\n+--------------------------------------------------------------------------------------------------------------------+\n| event_time                 | country    | city       | min(amou.. | max(amou.. | count(amount)        | sum(amou.. |\n+--------------------------------------------------------------------------------------------------------------------+\n| 2024-05-15 10:27:24.699000 | france     | lyon       | 56.23      | 56.23      | 1                    | 56.23      |\n+--------------------------------------------------------------------------------------------------------------------+\n1 row returned\n</code></pre> <p>The upper bound can be made inclusive with <code>incl</code>:</p> <pre><code>tektite&gt; (scan \"france\", \"lyon\" to \"france\", \"paris\" incl from\n             sales_by_country_city) -&gt;\n         (sort by country);\n+--------------------------------------------------------------------------------------------------------------------+\n| event_time                 | country    | city       | min(amou.. | max(amou.. | count(amount)        | sum(amou.. |\n+--------------------------------------------------------------------------------------------------------------------+\n| 2024-05-15 10:27:24.699000 | france     | lyon       | 56.23      | 56.23      | 1                    | 56.23      |\n| 2024-05-15 10:27:24.676000 | france     | paris      | 23.23      | 23.23      | 1                    | 23.23      |\n+--------------------------------------------------------------------------------------------------------------------+\n2 rows returned\n</code></pre> <p>The lower bound can be made exclusive with <code>excl</code>:</p> <pre><code>tektite&gt; (scan \"france\", \"lyon\" excl to \"france\", \"paris\" incl from\n             sales_by_country_city) -&gt;\n         (sort by country);\n+--------------------------------------------------------------------------------------------------------------------+\n| event_time                 | country    | city       | min(amou.. | max(amou.. | count(amount)        | sum(amou.. |\n+--------------------------------------------------------------------------------------------------------------------+\n| 2024-05-15 10:27:24.676000 | france     | paris      | 23.23      | 23.23      | 1                    | 23.23      |\n+--------------------------------------------------------------------------------------------------------------------+\n1 row returned\n</code></pre> <p>If you want to scan from a lower bound but don't want an upper bound you can use the special upper bound <code>end</code></p> <pre><code>tektite&gt; (scan \"germany\" to end from sales_by_country_city) -&gt;\n    (sort by country);\n+--------------------------------------------------------------------------------------------------------------------+\n| event_time                 | country    | city       | min(amou.. | max(amou.. | count(amount)        | sum(amou.. |\n+--------------------------------------------------------------------------------------------------------------------+\n| 2024-05-15 10:27:25.164000 | germany    | hamburg    | 56.23      | 56.23      | 1                    | 56.23      |\n| 2024-05-15 10:27:24.720000 | germany    | berlin     | 23.23      | 23.23      | 1                    | 23.23      |\n| 2024-05-15 09:18:25.535000 | uk         | manchester | 56.23      | 56.23      | 1                    | 56.23      |\n| 2024-05-15 09:18:25.557000 | uk         | london     | 76.23      | 85.23      | 2                    | 161.46     |\n| 2024-05-15 09:18:25.576000 | usa        | austin     | 23.23      | 23.23      | 1                    | 23.23      |\n| 2024-05-15 09:18:25.957000 | usa        | miami      | 56.23      | 56.23      | 1                    | 56.23      |\n+--------------------------------------------------------------------------------------------------------------------+\n6 rows returned\n</code></pre> <p>You can also use the special lower bound <code>start</code> to scan from the first key up to some upper bound:</p> <pre><code>tektite&gt; (scan start to \"uk\" incl from sales_by_country_city) -&gt; (sort by country);\n+--------------------------------------------------------------------------------------------------------------------+\n| event_time                 | country    | city       | min(amou.. | max(amou.. | count(amount)        | sum(amou.. |\n+--------------------------------------------------------------------------------------------------------------------+\n| 2024-05-15 10:27:24.699000 | france     | lyon       | 56.23      | 56.23      | 1                    | 56.23      |\n| 2024-05-15 10:27:24.676000 | france     | paris      | 23.23      | 23.23      | 1                    | 23.23      |\n| 2024-05-15 10:27:25.164000 | germany    | hamburg    | 56.23      | 56.23      | 1                    | 56.23      |\n| 2024-05-15 10:27:24.720000 | germany    | berlin     | 23.23      | 23.23      | 1                    | 23.23      |\n| 2024-05-15 09:18:25.535000 | uk         | manchester | 56.23      | 56.23      | 1                    | 56.23      |\n| 2024-05-15 09:18:25.557000 | uk         | london     | 76.23      | 85.23      | 2                    | 161.46     |\n+--------------------------------------------------------------------------------------------------------------------+\n6 rows returned\n</code></pre> <p>You can also scan <code>start</code> to <code>end</code>. This identical to using a <code>scan all</code> operator.</p> <pre><code>tektite&gt; (scan start to end from sales_by_country_city) -&gt; (sort by country);\n+--------------------------------------------------------------------------------------------------------------------+\n| event_time                 | country    | city       | min(amou.. | max(amou.. | count(amount)        | sum(amou.. |\n+--------------------------------------------------------------------------------------------------------------------+\n| 2024-05-15 10:27:24.699000 | france     | lyon       | 56.23      | 56.23      | 1                    | 56.23      |\n| 2024-05-15 10:27:24.676000 | france     | paris      | 23.23      | 23.23      | 1                    | 23.23      |\n| 2024-05-15 10:27:25.164000 | germany    | hamburg    | 56.23      | 56.23      | 1                    | 56.23      |\n| 2024-05-15 10:27:24.720000 | germany    | berlin     | 23.23      | 23.23      | 1                    | 23.23      |\n| 2024-05-15 09:18:25.557000 | uk         | london     | 76.23      | 85.23      | 2                    | 161.46     |\n| 2024-05-15 09:18:25.535000 | uk         | manchester | 56.23      | 56.23      | 1                    | 56.23      |\n| 2024-05-15 09:18:25.957000 | usa        | miami      | 56.23      | 56.23      | 1                    | 56.23      |\n| 2024-05-15 09:18:25.576000 | usa        | austin     | 23.23      | 23.23      | 1                    | 23.23      |\n+--------------------------------------------------------------------------------------------------------------------+\n8 rows returned\n</code></pre>"},{"location":"queries/#the-sort-operator","title":"The sort operator","text":"<p>This operator can only be used in queries, and is used to sort the data by a list of expressions. Each sort expression is applied from left to right.</p> <p>The expressions are standard Tektite expressions and support all the operators and functions, or custom WebAssembly functions that you can use in any expression.</p> <p>Some examples:</p> <pre><code>(scan all from latest_sensor_readings) -&gt;\n    (sort by country, to_upper(city))\n\n(scan all from cust_data) -&gt;\n    (sort by my_custom_wasm_function(cust_name, cust_address, cust_age))\n</code></pre>"},{"location":"queries/#legal-operators-in-queries","title":"Legal operators in queries","text":"<p>Queries don't support all the operators that you can use in static streams. The following operators are supported in queries:</p> <ul> <li><code>get</code></li> <li><code>scan all</code></li> <li><code>scan</code></li> <li><code>project</code></li> <li><code>filter</code></li> <li><code>sort</code></li> </ul> <p>Notably we do not support aggregations or joins for queries. The Tektite approach is to maintain joins and aggregations on the write path as static streams, and keep the read path (queries) lightweight.</p>"},{"location":"queries/#using-the-tektite-http-api-for-queries","title":"Using the Tektite HTTP API for queries","text":"<p>Queries can be executed at the CLI for ad-hoc purposes, but to execute queries from your application, you use the Tektite HTTP API, this also allows you to prepare statements to avoid the cost of setting up the query each time.</p> <p>If you're using golang in your application we also provide a simple golang client which allows you to execute and prepare statements and queries. This is a simple convenience wrapper around the HTTP API which you can use directly if you prefer.</p>"},{"location":"running_servers/","title":"Running Tektite servers","text":"<p>This guide assumes you have installed Tektite and the Tektite binaries directory is on the <code>PATH</code> environment variable</p>"},{"location":"running_servers/#starting-tektite-servers","title":"Starting Tektite servers","text":"<p>A tektite server is started using the <code>tektited</code> command. At minimum, it takes a path to a server configuration file:</p> <pre><code>&gt; tektited --config cfg/standalone.conf\n</code></pre> <p>The Tektite distribution contains a selection of server configuration files that can be used for different purposes, e.g. running a standalone server, or a cluster. They can be used as a starting point for your own installation.</p>"},{"location":"running_servers/#running-a-local-tektite-server-during-development","title":"Running a local Tektite server during development","text":"<p>When working with Tektite during development it's useful to spin up a Tektite server on your local machine to interact with. We provide a couple of pre-made configurations for working with standalone servers, depending on whether you want a persistent server or non-persistent one.</p>"},{"location":"running_servers/#running-a-local-non-persistent-server","title":"Running a local non-persistent server","text":"<p>The simplest way to spin-up a Tektite server is using the cfg/standalone.conf configuration file. This starts a Tektite server with a built-in non-persistent object store and cluster manager and doesn't require any other servers to be running. However, all data will be lost when the server is restarted.</p> <pre><code>&gt; tektited --config cfg/standalone.conf\n</code></pre>"},{"location":"running_servers/#running-a-local-persistent-server","title":"Running a local persistent server","text":"<p>At the low-level Tektite stores data in an object store like MinIO or Amazon S3. If you want a local development server but don't want to lose data when it restarts you can use a configuration that uses a local MinIO instance for persistence.</p> <p>First, please make sure MinIO is installed and started and you've created a bucket called <code>tektite-dev</code> and a secret key and access key.</p> <p>Now, edit <code>cfg/standalone-minio.conf</code> and enter your secret key and access key in the <code>minio-secret-key</code> and <code>minio-access-key</code> configuration properties.</p> <p>Now you can start a Tektite server with:</p> <pre><code>&gt; tektited --config cfg/standalone-minio.conf\n</code></pre>"},{"location":"running_servers/#shutting-down-tektite-servers","title":"Shutting down Tektite servers","text":"<p>A tektite cluster (including a persistent 'cluster' of one node) must always be shutdown using the <code>shutdown</code> option. You shouldn't shut down the whole cluster using <code>CTRL-C</code> or sending a KILL signal to the process. A tektite cluster stores data in memory which is asynchronously flushed to object storage. If you kill the whole cluster it's possible that you could lose data that has not been flushed yet.</p> <p>The <code>shutdown</code> option starts a clean shutdown process - it contacts each server in the cluster and ensures that it has flushed all data and performs other cleanup operations.</p> <p>When shutting down a cluster, you provide the same config that you used to start the cluster</p> <p>E.g.</p> <pre><code>&gt; tektited --config cfg/standalone-minio.conf --shutdown\n</code></pre> <p>Or</p> <pre><code>&gt; tektited --config cfg/cluster-minio.conf --shutdown\n</code></pre> <p>This can be run from any machine that has network access to the Tektite servers remoting listeners (as defined by the <code>cluster-addresses</code>  property in the server configuration).</p>"},{"location":"running_servers/#starting-a-tektite-cluster","title":"Starting a Tektite cluster","text":"<p>A tektite cluster requires an object store and <code>etcd</code> to be running.</p> <ul> <li>Object store. Tektite uses an object store such as MinIO or Amazon S3 to store data persistently. Please see the chapter on object stores for more information. For development purposes, MinIO is recommended as it's easy to set up locally.</li> <li>Tektite uses <code>etcd</code> so Tektite cluster nodes can agree about cluster membership. Tektite has a fairly lightweight usage of <code>etcd</code> and does not use it for  storing stream data.</li> </ul>"},{"location":"running_servers/#starting-a-cluster-for-local-development","title":"Starting a cluster for local development","text":""},{"location":"running_servers/#start-minio","title":"Start MinIO","text":"<p>First, please make sure MinIO is installed and started and you've created a bucket called <code>tektite-dev</code> and a secret key and access key.</p> <p>Now, edit <code>cfg/cluster-minio.conf</code> and enter your secret key and access key in the <code>minio-secret-key</code> and <code>minio-access-key</code> configuration properties.</p>"},{"location":"running_servers/#start-etcd","title":"Start etcd","text":"<p>Start etcd in standalone mode with:</p> <pre><code>&gt; etcd\n</code></pre>"},{"location":"running_servers/#start-tektite-nodes","title":"Start Tektite nodes","text":"<p>The config <code>cfg/cluster-minio.conf</code> is configured for three Tektite nodes.</p> <p>In separate consoles run:</p> <pre><code>&gt; tektited --config cfg/cluster-minio.conf --node-id 0\n2024-05-18 14:44:09.860009  INFO    tektite server 0 started\n</code></pre> <pre><code>&gt; tektited --config cfg/cluster-minio.conf --node-id 1\n2024-05-18 14:44:09.860025  INFO    tektite server 1 started\n</code></pre> <pre><code>&gt; tektited --config cfg/cluster-minio.conf --node-id 2\n2024-05-18 14:44:10.151445  INFO    tektite server 2 started\n</code></pre>"},{"location":"running_servers/#setting-up-a-tektite-cluster","title":"Setting up a Tektite cluster","text":""},{"location":"running_servers/#object-store","title":"Object store","text":"<p>Tektite currently supports the MinIO object store client which can be used with a MinIO cluster or Amazon S3.</p>"},{"location":"running_servers/#etcd","title":"etcd","text":"<p>An etcd cluster is required for managing cluster membership. It is not used for storing data. A single etcd cluster can be used many Tektite clusters</p>"},{"location":"running_servers/#configuring-tektite-servers","title":"Configuring Tektite servers","text":"<p>As a starting point you can base your server config on cfg/cluster-minio.conf</p> <p>MinIO is Amazon S3 API compatible so the MinIO client will work with S3 as well as MinIO clusters.</p> <p>Configure the following properties to connect to the object store you wish to use:</p> <ul> <li><code>minio-endpoint</code> - the address of the MinIO/S3 endpoint</li> <li><code>minio-access-key</code> - your access key</li> <li><code>minio-secret-key</code> = - your secret key</li> <li><code>minio-bucket-name</code> - the name of the bucket where Tektite data will be stored. You should use a separate bucket per \\ writable Tektite cluster</li> </ul> <p>Configure <code>cluster-name</code> to give each of your Tektite clusters a unique name.</p> <p>Configure <code>cluster-addresses</code>. These are the addresses at which Tektite nodes connect to each other for internal cluster traffic. They need to be accessible to each other, but they don't need to be accessible from the client side.</p> <p>Configure <code>http-api-addresses</code>. These are the addresses from which the Tektite HTTP API is served. This is used by the CLI and Tektite client applications.</p> <p>Configure <code>kafka-server-addresses</code>. These are the addresses that the Kafka server listens at. Kafka clients connect here.</p> <p>Configure <code>admin-console-addresses</code>. These are the addresses that the admin console is served from.</p> <p>Configure <code>cluster-state-manager-listen-addresses</code>. These are the addresses of your etcd nodes. Note that one etcd cluster can be shared by many Tektite clusters, as long as each Tektite cluster has a unique <code>cluster-name</code>.</p> <p>Other properties that you might commonly want to configure:</p> <ul> <li><code>processor-count</code> - this is the number of processors in the entire Tektite cluster. Typically, you would match this to <code>2 * (available cores on the machine) * number of nodes in cluster</code></li> </ul>"},{"location":"running_servers/#starting-tektite-cluster-nodes","title":"Starting Tektite cluster nodes","text":"<p>Start each node with the command line:</p> <pre><code>tektited --config cfg/cluster-minio.conf --node-id &lt;node_id&gt;\n</code></pre> <p>Where <code>&lt;node_id&gt;</code> is the unique id of each node. If there are <code>n</code> nodes in the cluster, node id should be in the range <code>0</code> to <code>n - 1</code>.</p>"},{"location":"topics/","title":"Topics","text":"<p>Tektite allows you to create topics and access them from any Kafka-compatible client. The topics have the same characteristics as they would in other Kafka compatible event streaming platforms - they are persistent and partitioned.</p> <p>With Tektite a topic is just a type of stream. It's a stream that takes input data from external Kafka producers and outputs data to external Kafka consumers.</p>"},{"location":"topics/#creating-topics","title":"Creating topics","text":"<p>The simplest way to create a topic is by using the <code>topic</code> operator.</p> <pre><code>my-topic := (topic partitions = 16)\n</code></pre> <p>This creates a topic called <code>my-topic</code> with 16 partitions. It's just a simple stream composed of a single <code>topic</code> operator.</p> <p>The number of partitions is mandatory when creating a topic.</p>"},{"location":"topics/#deleting-topics","title":"Deleting topics","text":"<p>Topics are deleted just like any stream:</p> <pre><code>delete(my-topic)\n</code></pre>"},{"location":"topics/#topic-magic","title":"Topic magic","text":"<p>The simple topic stream above is actually equivalent to the following stream:</p> <pre><code>my-stream := (kafka in partitions = 16) -&gt; (kafka out)\n</code></pre> <p>The <code>kafka in</code> operator exposes a Kafka compatible producer endpoint. I.e. it accepts messages from any Kafka compatible producer. The produced data is sent to a <code>kafka out</code> which stores it persistently and exposes a Kafka compatible consumer endpoint so the data can be consumed from any Kafka compatible consumer.</p> <p>The <code>topic</code> operator, under the hood, basically instantiates a <code>kafka in</code> followed by a <code>kafka out</code> as above. We provide a  distinct <code>topic</code> operator as creating a simple vanilla topic is a very common thing to do in Tektite.</p> <p>However, sometimes you want to do interesting things with topics that you can't do in existing event streaming platforms, and that's where a separate <code>kafka in</code> and <code>kafka out</code> operator become useful.</p>"},{"location":"topics/#filtered-topics","title":"Filtered topics","text":"<p>How about a server side filter?</p> <pre><code>filtered-topic :=\n    (kafka in) -&gt; (filter by matches(to_string(val), \"^hello\")) -&gt; (kafka out)\n</code></pre> <p>This creates a Kafka topic called <code>filtered-topic</code> which accepts any messages produced to it but only stores and exposes for consumption those where the message body starts with the string <code>\"hello\"</code>.</p>"},{"location":"topics/#exposing-an-existing-stream-to-kafka-consumers","title":"Exposing an existing stream to Kafka consumers","text":"<p>Let's say you have an existing stream in Tektite called <code>my-stream</code>. You can expose it to Kafka consumers using a <code>kafka out</code> operator.</p> <pre><code>exposed-stream := my-stream -&gt; (kafka out)\n</code></pre> <p>This will create a Kafka topic called <code>exposed-stream</code> which any Kafka compatible consumer can consume. Note that this  topic has no <code>kafka in</code> so you cannot produce to it. It's a read-only topic!</p> <p>Perhaps you have an existing topic living in your existing Apache Kafka installation - and you want to expose that to Kafka consumers after transforming it in some way. No problem!</p> <pre><code>upper-cased-topic :=\n    (bridge from my-kafka-topic props = (bootstrap.servers=\"...\")) -&gt;\n    (project key, hdrs, to_bytes(to_upper(to_string(val)))) -&gt;\n    (kafka out)\n</code></pre> <p>The possibilities are endless.</p>"},{"location":"topics/#write-only-topics","title":"Write only topics","text":"<p>Sometimes you want only to expose the Kafka producer endpoint, not a consumer endpoint.</p> <pre><code>latest_sensor_readings :=\n    (kafka in partitions = 32) -&gt;\n    (to table key = to_string(key))\n</code></pre> <p>This creates a topic called <code>latest-sensor-readings</code> that can only be produced to, as there is no <code>kafka out</code> operator.</p> <p>As data arrives it is stored in a table which is keyed on the bytes in the Kafka message key converted to a string. A table only stores the latest value for the key, so if the key was the sensor id of a IoT sensor this would store the latest message from each sensor.</p> <p>It could then be queried or used as input to other streams.</p> <p>Perhaps you just want to count the number of readings in the last 5 minutes grouped by sensor</p> <pre><code>readings_last_5_mins :=\n    (kafka in partitions = 16) -&gt;\n    (aggregate count(val) by to_string(key) size 5m hop 10s)\n</code></pre> <p>This would create a topic called <code>readings_last_5_mins</code> that can only be produced to. As messages arrive a windowed aggregation is maintained that counts the number of readings in the last 5 minutes, grouped by sensor.</p> <p>The results of the aggregation can be queried or used as input to another stream.</p> <p>The building blocks can be put together in many ways to do things with topics you never thought were possible.</p>"},{"location":"topics/#the-operator-schema","title":"The operator schema","text":"<p>Operators typically have an input and an output schema and the <code>topic</code> and <code>kafka in</code> and <code>kafka out</code> operators are no exception.</p> <p>The input and output schema are the same, and they model the structure of a Kafka message</p> <p>Here's the schema. Each column is written as <code>column_name:column_type</code></p> <pre><code>offset: int\nevent_time: timestamp\nkey: bytes\nval: bytes\nhdrs: bytes\n</code></pre> <p>The <code>offset</code> column corresponds to the Kafka message offset in the partition. It's assigned by Tektite on receipt of the message before it is output from the <code>kafka out</code> operator.</p> <p>The <code>event_time</code> column corresponds to the timestamp of the Kafka message. This can be assigned on the client or on the server depending on configuration (see <code>kafka-use-server-timestamp</code> configuration property). In Tektite all streams (not just topics) have an <code>event_time</code>.</p> <p>The <code>key</code> field corresponds to the key of the Kafka message. It is an arbitrary byte string. It is optional and can be <code>null</code></p> <p>The <code>val</code> field corresponds to the body of the Kafka message. It is also a byte string.</p> <p>The <code>hdrs</code> field corresponds to the raw headers of the Kafka message. It is a byte string and can be <code>null</code></p> <p>An operator that takes input from a <code>kafka in</code> operator will receive data with this schema. Similarly, the input to a  <code>kafka out</code> operator must have this schema, or the Kafka consumer won't be able to understand it.</p>"},{"location":"topics/#using-expressions-to-extract-typed-data","title":"Using expressions to extract typed data","text":"<p>Commonly, your Kafka messages will contain structured data, e.g. in JSON format, and you want to perform filters or projections based on fields in that data.</p> <p>Let's say your messages are from IoT sensors. The message key is the <code>sensor_id</code> encoded as a UTF-8 string. The message body is JSON with the following format:</p> <pre><code>{\n  \"country\": \"UK\",\n  \"area_code\": 1234,\n  \"temperature\": 23.56\n}\n</code></pre> <p>The message also contains a header called <code>model_version</code> containing the version of the sensor.</p> <p>We want a topic that only stores messages from the UK from sensors with version = \"12.23\". We can use a filter with an expression that extracts values from the message.</p> <p>The function library contains functions such as <code>json_string</code> and <code>json_int</code> which extract the named JSON field from the named column in the schema. There's also a function <code>kafka_header</code> which extracts the named Kafka header from the raw headers.</p> <pre><code>filtered_readings :=\n    (kafka in partitions = 16) -&gt; \n    (filter by json_string(\"country\", val) == \"UK\" &amp;&amp;\n        to_string(kafka_header(\"model_version\", hdrs)) == \"12.23\") -&gt;\n    (kafka out)\n</code></pre> <p>Or perhaps we have a simple topic, and we want to hang an aggregation of it:</p> <pre><code>my_readings := (topic partitions = 16)\n\nsensor_readings_by_country :=\n   my_readings -&gt; \n       (project json_string(\"country\", val) as country,\n                json_int(\"area\", val) as area,\n                json_float(\"temperature\") as temp) -&gt;\n       (aggregate max(temp), min(temp), avg(temp) by country, area)        \n</code></pre> <p>Other times when you have an existing stream that you want to expose as a Kafka consumer endpoint, you want to convert the schema of the existing stream into the schema that the <code>kafka out</code> operator requires. You can use a projection to do this.</p> <p>For example, let's say you have an existing stream <code>sales</code> with the schema</p> <pre><code>tx_id: string\ncust_id: string\nproduct_id: string\namount: int\nprice: decimal(10, 2)\n</code></pre> <p>And you want to convert this into an output topic where the key of the Kafka message is the <code>tx_id</code> and the message body is JSON of the form:</p> <pre><code>{\n  \"id\": \"tx12345\",\n  \"customer_id\": \"cust46464\",\n  \"product_id\": \"prod67676\",\n  \"amount\": 23,\n  \"price\": \"99.99\"\n}\n</code></pre> <p>Then you can create a new stream that exposes the data to Kafka consumers by using the built-in <code>sprintf</code> function to  format the JSON string</p> <pre><code>transactions := ... // existing stream\n\ntransactions_out := transactions -&gt;\n   (project to_bytes(tx_id), \n            to_bytes(sprintf((\"{\\\"id\\\": %s,\n              \\\"customer_id\\\": %s,\n              \\\"product_id\\\": %s,\n              \\\"amount\\\": %d,\n              \\\"price\\\": %s}\",\n                tx_id,\n                cust_id,\n                product_id,\n                amount,\n                to_string(price))),\n            null) -&gt;\n   (kafka out)         \n</code></pre> <p>The input schema to the <code>kafka out</code> operator requires <code>[key:bytes, val:bytes, hdrs:bytes]</code> and that's what the <code>project</code> operator will output as it has three expressions each of which return a value of type <code>bytes</code>.</p>"},{"location":"topics/#data-retention","title":"Data retention","text":"<p>If you don't want to keep the data in your topic forever, you can set a maximum retention time on it. Data will be deleted asynchronously from the topic once that time has been exceeded.</p> <p>This will create a topic that keeps data up to 7 days:</p> <pre><code>topic-with-retention := (topic partitions = 16 retention = 7d)\n</code></pre> <p>The duration string is of the form of a positive integer followed by a symbol <code>ms</code> = milliseconds, <code>s</code> = seconds, <code>m</code> = minutes, <code>h</code> = hours, <code>d</code> = days.</p> <p>Retention can also be specified on a <code>kafka out</code> operator</p> <pre><code>filtered :=\n    (kafka in partitions = 16) -&gt;\n    (filter by len(val) &gt; 1000) -&gt;\n    (kafka out retention = 2h)\n</code></pre>"},{"location":"topics/#generating-watermarks","title":"Generating watermarks","text":"<p>Watermarks are automatically generated at a <code>kafka in</code> operator. Please see the section on generating watermarks for  more information.</p>"},{"location":"union/","title":"Unions","text":"<p>The <code>union</code> operator is used to merge multiple streams into a single stream.</p> <p>Let's say you have 4 different stock feed streams; <code>nasdaq-ticker</code>, <code>nyse-ticker</code>, <code>euro-ticker</code>, <code>lse-ticker</code>, each one for different exchanges. Each feed stream has the following schema:</p> <pre><code>event_time: timestamp\nticker: string\nprice: decimal(10, 4)\n</code></pre> <p>The following would combine them into a single stream:</p> <pre><code>combined-stream :=\n    (union nasdaq-ticker, nyse-ticker, euro-ticker, lse-ticker) -&gt;\n    (store stream)\n</code></pre> <p>The <code>union</code> operator requires that all input streams have the same column types and number of columns. If that is not the case a projection should be applied to the input before it is sent to the <code>union</code>.</p> <p>Lets sat the <code>nyse-ticker</code> actually had the following schema:</p> <pre><code>event_time: timestamp\nticker: string\nprice: float\n</code></pre> <p>So we need to convert the <code>price</code> field to a <code>decimal(10, 4)</code>:</p> <pre><code>nyse-adapted := nyse-ticker -&gt;\n   (project symbol, to_decimal(tick, 10, 4))\n\ncombined-stream :=\n    (union nasdaq-ticker, nyse-adapted, euro-ticker, lse-ticker) -&gt;\n    (store stream)\n\n</code></pre> <p>The <code>union</code> output column names are taken from the column names of the first input.</p>"},{"location":"wasm/","title":"Implementing custom functions using WebAssembly","text":"<p>You can implement custom functions using WebAssembly (WASM) and use them in any expression, just like built-in functions.</p> <p>WebAssembly is a binary execution format originally designed for executing high performance code in browsers  but is now emerging as a popular way for components written in different languages to interoperate.</p> <p>You write your function in your chosen language and compile to a WASM module. You then create a JSON descriptor file which describes the functions that you want to expose, and then register the module with Tektite. Then you can use your function(s) in Tektite just like any built-in function.</p> <p>Let's walk through an example.</p>"},{"location":"wasm/#write-and-compile-the-module","title":"Write and compile the module","text":"<p>Note: WASM only supports a very limited set of types for function arguments and return values - basically just ints and floats. There is no direct support for more 'complex' types such as strings and byte arrays. The usual way around this is write the arguments / return values into WASM linear memory (which is accessible by both the module and the host) and encode a 'pointer' to that memory as an int which is passed to/from the function. The way this is done in practice is not standardised and depends on the particular programming language. Until a standard way to pass 'complex' types is provided in WASM (perhaps the Component Model will solve this) it's the best we can do.</p>"},{"location":"wasm/#create-a-module-in-golang","title":"Create a module in golang","text":"<p>For writing golang WASM modules, we recommend using TinyGo. This has the advantage of built-in support for compilation to WASM and it creates small binaries. Please follow the instructions on the TinyGo website for installing it.</p> <p>Let's compile an example golang WASM module from the repository. You will need both the files: <code>my_mod.go</code> and <code>utils.go</code></p> <p>The former contains our WASM function, the latter contains some boilerplate utility functions used for converting WASM arguments and return values to/from complex types such as <code>string</code> and <code>[]byte</code>.</p> <p>Our WASM function is called <code>repeatString</code> - it takes a <code>string</code> parameter called <code>str</code> and an <code>int</code> parameter called <code>times</code> and returns the string, repeated <code>times</code> times.</p> <p>To compile it, from this directory execute:</p> <pre><code>tinygo build -o my_mod.wasm -scheduler=none -target=wasi ./\n</code></pre> <p>This will compile it to a WASM binary called <code>my_mod.wasm</code></p>"},{"location":"wasm/#create-the-json-descriptor","title":"Create the JSON descriptor","text":"<p>Next, we need to create a JSON descriptor file for the module. This tells Tektite which functions from the module you  want to import into Tektite, and what the Tektite data types are for their parameters and return values.</p> <p>This is necessary because Tektite cannot infer the Tektite function signature from the WASM function signature because a WASM function with a particular signature could map to many different function signatures in Tektite. E.g. a WASM function that takes an i64 argument - this could represent an integer, or it could be a pointer to a <code>string</code> or a pointer to a <code>[]byte</code>, etc.</p> <p>The JSON descriptor for our module looks like this:</p> <pre><code>{\n    \"name\": \"my_mod\",\n    \"functions\": {\n        \"repeatString\": {\n            \"paramTypes\": [\"string\", \"int\"],\n            \"returnType\": \"string\"\n        }\n    }\n}\n</code></pre> <p>The name of the module is specified, and each function is listed along with the Tektite types of its parameters and  return value.</p> <p>The JSON descriptor should be saved into a file called <code>my_mod.json</code> - i.e. the name of the module, with the <code>.json</code> file  extension.</p>"},{"location":"wasm/#register-the-module","title":"Register the module","text":"<p>To upload the module using the Tektite CLI, you use the <code>register_wasm</code> command. This takes a path to the <code>.wasm</code> file. It can be absolute or relative to the current working directory where the CLI was run from.</p> <p>In the directory there must be the <code>.wasm</code> file and the corresponding <code>.json</code> descriptor file.</p> <pre><code>tektite&gt; register_wasm(\"examples/wasm/my_mod.wasm\");\n</code></pre> <p>You can also register WASM modules using the HTTP API and using the golang client.</p>"},{"location":"wasm/#create-a-stream-that-uses-the-module","title":"Create a stream that uses the module","text":"<p>Now that we've registered our module, our function <code>repeatString</code> should now be available. Let's try it.</p> <p>At the CLI, we'll create a Kafka topic that takes the body of the Kafka message and applies the <code>repeatString</code> function to it to \"multiply\" the message body by 3 before exposing the message for consumption.</p> <pre><code>tektite&gt; repeat-topic :=\n    (kafka in partitions = 16) -&gt;\n    (project key, hdrs, to_bytes(my_mod.repeatString(to_string(val), 3))) -&gt;\n    (kafka out);\n</code></pre> <p>Note that when we use our <code>repeatString</code> function it is scoped by the name of our WASM module <code>my_mod</code>, giving <code>my_mod.repeatString</code>.  All custom functions must be called this way.</p> <p>Now, using kcat we can open two consoles to consume and produce a message to this topic:</p> <p>In one console, start a consumer:</p> <pre><code>kcat -q -b 127.0.0.1:8880 -G mygroup repeat-topic\n</code></pre> <p>In another console, send a messsage:</p> <pre><code>echo \"hello how are you?\" | kcat -b 127.0.0.1:8880 -t repeat-topic -P\n</code></pre> <p>In the consumer console you should see:</p> <pre><code>hello how are you?hello how are you?hello how are you?\n</code></pre>"},{"location":"wasm/#unregister-a-module","title":"Unregister a module","text":"<p>To unregister a module you use the <code>unregister_wasm</code> command. This takes the module name (not a path or file name) to unregister, in this case <code>my_mod</code>.</p> <p>Before unregistering a module make sure you have deleted any streams that use the module, or they will fail when processing data.</p> <pre><code>tektite&gt; unregister_wasm(\"my_mod\");\n</code></pre>"},{"location":"watermarks/","title":"Watermarks","text":"<p>Watermarks are used in Tektite for determining when aggregation windows can be closed.</p> <p>Watermarks are generated at the points where data enters the system which are the <code>kafka in</code> and <code>topic</code> operators for Kafka topics hosted in Tektite or the <code>bridge from</code> operator for data entering from an external topic.</p> <p>A watermark carries a timestamp and flows with the rest of the data through the network of streams along all the routes that data can flow.  When an operator receives a watermark with a particular timestamp, the watermark tells the operator that it shouldn't expect to see any more data with a timestamp less than this value.</p> <p>As watermarks flow through partitions, joins and unions - all of which can receive data from different processors, the watermark is delayed until a watermark has been received from each sending processor, and then a single watermark with a value of the minimum of the  incoming watermarks is forwarded.</p> <p>Internally, watermarks piggyback on a closely related Tektite concept: barriers. Barriers are periodically injected into streams at sources and flow through the network of streams. They're used to guarantee snapshot isolation.</p> <p>When a watermark reaches a windowed aggregation, the <code>aggregate</code> operator makes a decision about which open windows (if any) should be closed and have results emitted for them.</p> <p>Tektite supports two strategies for generating watermarks.</p>"},{"location":"watermarks/#event-time-watermark","title":"Event time watermark","text":"<p>With an event time strategy, the value given to the watermark when injected at the source is the maximum value of any data seen so far at the source for the processor minus the value of <code>watermark_lateness</code>. The watermark is based on the actual timestamp of the message which often does not correspond to when the message is processed.</p> <p>As an example, let's say the maximum timestamp a <code>kafka in</code> operator has seen for incoming produced messages is  <code>2024-05-16 09:00:00</code> and <code>watermark_lateness</code> is set to <code>5s</code>. Then the next watermark to be generated will have a timestamp value of <code>2024-05-16 08:59:55</code>.</p> <p>This watermark will then flow through the system, and if it reaches a windowed aggregation the aggregation may decide to close all open windows with a window end &lt;= <code>2024-05-16 08:59:55</code> (depending on configuration)</p> <p>Event time watermarks are the default type and will be injected in any <code>kafka in</code>, <code>topic</code> or <code>bridge from</code> operators automatically if not explicitly configured.</p> <p>Here's an example of explicitly configuring a <code>topic</code> with event time watermarks:</p> <pre><code>my_topic :=\n    (topic partitions = 16\n     watermark_type = event_time\n     watermark_lateness = 10s\n     watermark_idle_timeout = 30s)\n</code></pre> <p>The same parameters can be used on <code>kafka in</code> and <code>bridge from</code>.</p> <p>With an event time strategy, if no data is received on a processor then watermark value can get stuck, and this could  prevent downstream windowed aggregations from closing.</p> <p>To unstick stuck watermarks a special watermark with a value of <code>-1</code> is generated if no data is received for <code>watermark_idle_timeout</code>. If a windowed aggregation receives a watermark of <code>-1</code> its upstreams are all idle, and can emit results.</p> <p>The default value of <code>watermark_lateness</code> is <code>1s</code>. The default value of <code>watermark_idle_timeout</code> is <code>1m</code></p>"},{"location":"watermarks/#processing-time-watermark","title":"Processing time watermark","text":"<p>With a processing time strategy, the value given to the watermark when injected at the source is based on the processing time (i.e. the current system time on the server) minus the value of <code>watermark_lateness</code>.</p> <p>Here's an example of configuring a <code>bridge from</code> with processing time watermarks:</p> <pre><code>sales-imported :=\n    (bridge from sales partitions = 16 props = (\"bootstrap.servers\" = \"mykafka.foo.com:9092\")\n        watermark_type = processing_time\n        watermark_lateness = 10s) -&gt;\n    (store stream)            \n</code></pre>"}]}